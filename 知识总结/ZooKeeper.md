# ZooKeeper

---

layout: post
title: "分布式中的 ZooKeeper"
date: 2021-08-09 22:46
comments: true
tags: 
	- 分布式
	- 知识总结

---

ZooKeeper 是一个开放源码的分布式应用程序协调服务，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，**随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降**。

有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个 zookeeper 最新的 zxid。

## 1. zk 提供了什么

### 1.2 文件系统

#### 1.2.1 特性

Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，zk 每个节点都可以存放值，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 znode 不能用于存放大量的数据，每个节点的存放数据上限为**1M**。

#### 1.2.2 znode 四种类型

1. **PERSISTENT 持久化目录节点**
   客户端与 zookeeper 断开连接后，该节点依旧存在。
2. **PERSISTENT_SEQUENTIAL 持久化顺序编号目录节点**
   客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号。
3. **EPHEMERAL 临时目录节点**
   客户端与 zookeeper 断开连接后，该节点被删除。
4. **EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点**
   客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号。

### 1.3 通知机制

client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。

## 2. zk 可以用来做什么

### 2.1 命名服务

命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的服务器，提供的服务的地址，或者一个远程的对象等等。

即在特定 znode 下存放服务或资源地址，以 znode 路径作为名字。

### 2.2 配置管理

程序分布式的部署在不同的机器上，将程序的配置信息放在 zk 的 znode 下，当 znode 中存放的配置发生改变时，利用 watcher 通知给各个客户端，从而更改配置。

### 2.3 集群管理

所谓集群管理无在乎两点：是否有机器退出和加入、选举 master。

- **判断是否有服务器加入或退出时**：所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zk 的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除。新机器加入也是类似，所有机器收到通知：新兄弟目录加入。
- **选举 master 时**：所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为 master 就好。

### 2.4 分布式锁

有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

- **保持独占**：我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的 distribute_lock 节点就释放出锁。
- **控制时序**：/distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选 master 一样，编号最小的获得锁，用完删除，依次方便。

### 2.5 队列管理

- 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。
  **达成方法**：在约定目录下创建临时目录节点，监听子节点数目是否是我们要求的数目。
- FIFO 队列。
  **达成方法**：和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建持久化顺序编号目录节点，创建成功时 Watcher 通知等待的队列，队列删除序列号最小的节点用以消费。此场景下 Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消息内容，节点序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。

## 3. zk 工作原理

### 3.1 事务的顺序一致性

zookeeper 采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch 用来标识 leader 是否发生改变，如果有新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。

### 3.2 集群数据复制

Zookeeper 作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。

#### 3.2.1 集群数据复制类别

从客户端读写访问的透明度来看，数据复制集群系统分下面两种：

1. 写主 (Write Master) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称**读写分离**；
2. 写任意 (Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。

对 zookeeper 来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多写吞吐能力肯定下降（这也是它建立 observer 的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。

#### 3.2.2 数据复制原理

Zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。

### 3.3 server 工作状态

LOOKING：当前 server 不知道 leader 是谁，正在搜寻
LEADING：当前 server 即为选举出来的 leader
FOLLOWING：leader 已经选举出来，当前 server 与之同步
OBSERVING：观察状态，同步 leader 状态，不参与投票

### 3.4 选举流程

当 leader 崩溃或者 leader 失去大多数的 follower，这时 zk 进入恢复模式，恢复模式需要重新选举出一个新的 leader，让所有的 Server 都恢复到一个正确的状态。zk 的选举算法是基于 fast paxos 算法实现的。

选举胜负策略：

- 优先检查 ZXID。ZXID 比较大的服务器优先作为 Leader。
- 如果 ZXID 相同，那么就比较 myid。myid 较大的服务器作为 Leader 服务器。

#### 3.4.1 basic paxos

1. 选举线程由当前 Server 发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的 Server；

2. 选举线程首先向所有 Server 发起一次询问 (包括自己)；

3. 选举线程收到回复后，验证是否是自己发起的询问 (验证 zxid 是否一致)，然后获取对方的 id(myid)，并存储到当前询问对象列表中，最后获取对方提议的 leader 相关信息 (id, zxid)，并将这些信息存储到当次选举的投票记录表中；

4. 收到所有 Server 回复以后，就计算出 zxid 最大的那个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server；

5. 线程将当前 zxid 最大的 Server 设置为当前 Server 要推荐的 Leader，如果此时获胜的 Server 获得 n/2 + 1 的 Server 票数，设置当前推荐的 leader 为获胜的 Server，将根据获胜的 Server 相关信息设置自己的状态，否则，继续这个过程，直到 leader 被选举出来。 

通过流程分析我们可以得出：要使 Leader 获得多数 Server 的支持，则 Server 总数必须是奇数 2n+1，且存活的 Server 的数目不得少于 n+1. 每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。

<img src="https://segmentfault.com/img/bV8XeP?w=357&h=791" alt="clipboard.png" style="zoom:100%;" />

#### 3.4.2 fast paxos

fast paxos 流程是在选举过程中，某 Server 首先向所有 Server 提议自己要成为 leader，当其它 Server 收到提议以后，解决 epoch 和 zxid 的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出 Leader。

<img src="https://segmentfault.com/img/bV8XeR?w=533&h=451" alt="clipboard.png" style="zoom:100%;" />

### 3.5 同步流程

选完 Leader 以后，zk 就进入状态同步过程。

1. Leader 等待 server 连接；
2. Follower 连接 leader，将最大的 zxid 发送给 leader；
3. Leader 根据 follower 的 zxid 确定同步点；
4. 完成同步后通知 follower 已经成为 uptodate 状态；
5. Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。

### 3.6 leader 的作用

在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。

所有事务请求必须由一个全局唯一的服务器来协调处理，这个服务器就是 Leader 服务器，其他的服务器就是follower。leader 服务器把客户端的事务请求转化成一个事务 Proposal（提议），并把这个 Proposal 分发给集群中的所有 Follower 服务器。之后 Leader 服务器需要等待所有Follower 服务器的反馈，一旦超过半数的 Follower 服务器进行了正确的反馈，那么 Leader 就会再次向所有的Follower 服务器发送 Commit 消息，要求各个 follower 节点对前面的一个 Proposal 进行提交;

### 3.7 宕机处理

Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。

如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；

如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。

zk 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 zk 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。

### 3.8 zk 与 CP 原则

zk 遵循的是 CP 原则，即保证一致性和网络分区容错性，但不保证可用性。体现在哪里呢？

当 Leader 宕机后，zk 集群会发起新一轮投票选举，投票选举期间所有的 Follower 主机都处于 LOOKING 状态，对外不提供服务。但 Leader 的选举一般在 200ms 内完成，最长不超过60s，整个选举期间，zk 集群是不对外提供服务的，不接受客户端的读写请求的，即 zk 集群处于瘫痪状态。所以它不满足可用性。
