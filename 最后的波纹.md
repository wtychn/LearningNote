# 面经
# MySQL事务

​		事务指逻辑上的一组操作，组成这组操作的各个单元，要不全部成功，要不全部不成功。 MySQL事务有四个特性，分别是原子性，持久性，隔离性，一致性，即ACID。原子性指的是整个事务是不可分割的最小单位，整个事务要么全部执行成功，要么全部执行失败。持久性指的是事务一旦提交，对数据库就是永久性的影响，不会因为宕机等故障导致数据丢失，隔离性指的是每个事务都感觉不到系统有其他事务在并发地执行，而一致性指的是事务将数据库从一种状态转变为另一种状态，在事务的前后数据库的完整性约束不会被破坏。

​		MySQL事务支持是在引擎层实现，而非服务层，所以不同引擎对事务的实现方式各不相同，有些引擎比如MyIsam不支持事务。

​		对于InnoDB引擎来说，原子性是使用undolog实现的。undolog，即回滚日志，用于记录数据被修改前的信息.undolog有两个作用，一个是提供回滚来实现原子性，另一个是实现多版本控制，MVCC。undolog主要记录数据的逻辑变化。在数据被修改的时候，比如delete,undolog中会记录一条对应的insert记录，反之亦然。当执行回滚时，就可以从undolog中的逻辑记录读取到相应的内容并进行回滚。在MVCC中，也是通过undolog实现的，当读取的某一行被其他事务锁定时，它可以从undolog中分析出该行记录以前的数据时什么。

​		undolog 是用段（segment)来存放的来记录的，每个undo操作在记录的时候占用一个undo log segment。在事务提交的时候，innodb不会立即删除undolog,因为后续还可能用到，比如在隔离级别为repeadable read时，事务读取的都是开启事务时最新提交的版本，只要该事务不结束，该行版本就不能删除。但是在事务提交的时候，会将该事务对应的undo log放入到删除列表中，后续通过purge线程删除。 并且提交事务时， 还会判断undo log分配的页是否可以重用，如果可以，会分配给后面来的事务，避免每个独立的事务分配独立的undolog而浪费空间。

​			通过undo log记录delete和update操作的结果发现：(insert操作无需分析，就是插入行而已)

- delete操作实际上不会直接删除，而是将delete对象打上delete flag，标记为删除，最终的删除操作是purge线程完成的。
- update分为两种情况：update的列是否是主键列。
  - 如果不是主键列，在undo log中反向记录是如何update的。即update是直接进行的。
  - 如果是主键列，update分两部执行：先删除该行，再插入一行目标行。

值得一提的时，undolog也需要持久化，产生redolog。

​		持久性使用redolog来实现。redolog是重做日志，通常是物理日志，记录的是数据页的物理修改，而不是某一行操作的逻辑修改。redolog包括两个部分，一是内存中的日志缓冲log buffer，这部分日志是易失性的，另一个是磁盘上的重做日志redolog file，该部分是持久的。InnoDB通过force log at commit 机制实现事务的持久性，即在事务提交的时候，必须先将事务的所有事务写入到磁盘上的redolog file进行持久化。因为如果先写数据库再写日志，如果出现宕机，数据的操作将是不可恢复的。由于mysql进程mysqld在用户空间，log buffer在用户空间的内存中，将数据从内存写入磁盘中间需要经过操作系统内核空间的os buffer,然后调用一次系统的fsync()函数，调用这个函数需要cpu从用户态转换为内核态，经常处理开销较大。

​		所以MySQL支持用户自定义在用户提交时如何将redolog buffer刷入到redolog file中，利用变量innodb_flush_log_at_trx_commit的值决定。这个变量有三个值0，1，2。代表了3中方法。

- 当设置为1的时候，事务***每次***提交都会将log buffer中的日志写入os buffer并调用fsync()刷到log file中。这种方式不会丢失任何数据，但是IO的性能较差。
- 当设置为0的时候，事务提交时不会写入，而是***每秒* **写入os buffer并调用fsync()写入到log file中。当系统崩溃，会丢失1秒钟的数据。
- 当设置为2的时候，***每次***提交都仅写入到os buffer，然后是***每秒***调用fsync()将os buffer中的日志写入到log file。

​        SQL标准中定义了4个隔离级别，分别是未提交读，提交读，可重复读和可串行化。隔离级别依次逐渐提高。在未提交读的级别中，在一个事务中对数据进行修改，即使这个事务还没有提交，其他事务也可以看到这个修改，这也被称为脏读。（这个事务修改数据）

​        在提交读中，只允许读取已经提交的数据。但是如果在一个事务内对数据进行两次读取，这时候其他事务有可能修改了这个数据并提交。这个事务两次读取的数据是不一样的，这个级别又被成为不可重复读。这也是大多数数据库的默认级别。（其他事务修改数据）

​        在可重复读中，保证了在同一个事务中多次读取同样的记录结果是一致的。但是当一个事务在读取某一个范围的数据时，其他事务在该范围内插入了一条新的记录，这个事务再进行读取时会产生幻行，也就是幻读现象。InnoDB引擎默认的隔离级别就是可重复读，并且通过MVCC解决了幻读现象。

​        可串行化是最高的隔离级别，它通过强制事务串行执行，避免了幻读现象。但同时可串行化会在读取的每一行数据都加上锁，可能导致大量的超时和锁争用问题。

​        隔离性主要使用加锁和MVCC（多版本并发控制）实现。MyISAM引擎只支持表级锁，InnoDB行级锁表级锁都支持，默认情况下在允许使用行级锁地时候都会使用行级锁，表级锁只有两种简单的锁类型：

- 共享锁，允许一个事务去读一行，即读锁。
- 独占锁，允许一个事务更新或删除一行，即写锁。

行级锁支持更多的锁类型：

- 意向共享锁， 事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该**表**的IS锁。即获取低级别共享锁的同时，在高级别上也获取特殊的共享锁 。
- 意向独占锁， 事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该**表**的IX锁。即 获取低级别独占锁的同时，在高级别上也获取特殊的独占锁 。

低级别锁表示的是行锁或页锁，意向锁可能是多条记录组成的范围锁，也可能直接就是表意向锁。 

意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存。 

InnoDB有三种锁算法：

- record lock （记录锁） ：即行锁,它的行锁锁定的是key,基于唯一性索引键列来锁定,如果没有唯一性索引键列，则会自动在隐式列上创建索引并完成锁定。
- gap lock （间隙锁） ：范围锁，但是不锁定行记录本身。
- next-key lock：范围锁加行锁，即范围锁并锁定记录本身，gap lock + record lock。innodb对行的锁申请默认都是这种算法。如果有索引，则只锁定指定范围内的索引键值，如果没有索引，则自动创建索引并对整个表进行范围锁定。之所以锁定了表还称为范围锁定，是因为它实际上锁的不是表，而是把所有可能的区间都锁定了，从主键值的负无穷到正无穷的所有区间都锁定，等价于锁定了表。

有了锁我们就可以实现四种隔离级别，但由于加锁的开销比较大，InnoDB实际上是使用锁+MVCC的方式实现的隔离级别。

​        在InnoDB中，给每行增加两个隐藏字段来实现MVCC，一个用来记录数据行的创建时间，另一个用来记录行的过期时间（删除时间）。在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 

默认的隔离级别（REPEATABLE READ）下，增删查改变成了这样：

- SELECT：读取创建版本小于或等于当前事务版本号，并且删除版本为空或大于当前事务版本号的记录。这样可以保证在读取之前记录是存在的。
- INSERT：将当前事务的版本号保存至行的创建版本号
- UPDATE：新插入一行，并以当前事务的版本号作为新行的创建版本号，同时将原记录行的删除版本号设置为当前事务版本号
- DELETE：将当前事务的版本号保存至行的删除版本号。

（快照读：读取的是快照版本，也就是历史版本

   当前读：读取的是最新版本）

​         普通的SELECT使用的是快照读并且不加锁。而对于锁定读、UPDATE和DELETE，使用当前读，需要加锁，至于加什么锁视情况而定。如果你使用了唯一索引作为检索条件，那么只需锁定索引记录，用记录锁即可；如果没有，或者用到了索引范围扫描，那么将会使用间隙锁或者next-key锁以此来阻塞其它会话向这个范围内的间隙插入数据。 

​		所以，InnoDB通过MVCC实现可重复读，并使用间隙锁解决了大部分的幻读的问题。但是有些情况是不能避免幻读的，比如事务A先select一个范围，事务B在这个范围中insert一个数据，这时候事务B会加一个gap锁，但是当事务B读取后，这个gap锁就会释放掉。接着事务A不加条件的进行UPDATE，作用到所有的行上，这时候事务A再次进行select就会查询到事务B插入的新行。只有将隔离级别提高到可串行化，才可以完全解决幻读问题。

通过保证数据库原子性、持久性、一致性的实现，数据库事务的一致性得以保证。





# MYSQL索引

索引是用于快速找到记录的一种数据结构。在MySQL中，索引是在存储引擎层而不是服务器层实现的，不同引擎的索引实现方式也不一样。最常见的索引就是B树索引。B 树又叫平衡多路查找树。一棵m阶的B 树 (m叉树)的特性：

- B树中所有节点的孩子节点数中的最大值称为B树的阶，记为M（**重点**）
- 树中的每个节点至多有M棵子树 ---即：如果定了M，则这个B树中任何节点的子节点数量都不能超过M
- 若根节点不是终端节点，则至少有两棵子树
- 除根节点和叶节点外，所有点至少有m/2棵子树（上溢）
- 所有的叶子结点都位于同一层。
-  所有节点关键字是按递增次序排列，并遵循左小右大原则； 

​        使用Ｂ树作为索引是因为磁盘IO问题，数据库的数据一般都存放在磁盘里，磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来。如果IO操作的效率很低，那么，当在大量数据存储中，查询时我们不能一下子将所有数据加载到内存中，只能逐一加载磁盘页，每个磁盘页对应树的节点。造成大量磁盘IO操作（最坏情况下为树的高度）
 　   所以，我们为了减少磁盘IO的次数，就你必须降低树的深度，将“瘦高”的树变得“矮胖”。

​        而InnoDB所采用的是B+树作为索引， B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。 

它与B树的差异在于：

- 有k个叶子结点的结点必然有k个关键码；
- 非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。
- 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录

B+树对于B树的优点是：

1、B+**树的层级更少**：相较于B树B+每个**非叶子**节点存储的关键字数更多，树的层级更少所以查询数据更快；

2、B+**树查询速度更稳定**：B+所有关键字数据地址都存在**叶子**节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定;

3、B+**树天然具备排序功能：**B+树所有的**叶子**节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。

4、B+**树全节点遍历更快：**B+树遍历整棵树只需要遍历所有的**叶子**节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。

**B树**相对于**B+树**的优点是，如果经常访问的数据离根节点很近，而**B树**的**非叶子**节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比**B+树**快。

因为B树这样的特性，B树索引适用于

全值匹配：和索引中所有的列进行匹配

匹配最左前缀：只是用索引的第一列进行匹配

匹配列前缀：只匹配第一列的值的开头部分

匹配范围值：匹配第一列的一个范围

精确匹配某一列并范围匹配某一列：第一列全匹配，第二列范围匹配

以及只访问索引，即覆盖索引的查询。

由此索引将大大减少了服务器需要扫描的数据量，可以帮助服务器避免排序和临时表，并可以将随机I/O变成顺序I/O。

而B树索引也有一些限制：

- 如果不是按照索引的最左列开始查找，无法匹配对应的叶节点，就没办法使用索引。

- 不能跳过索引中的列。

- 如果查询中有某个索引列的范围查询，则其右边素有的列都无法使用索引查找



我们以InnoDB所使用的B+树来介绍索引：

InnoDB中有页的概念，页是其磁盘管理的最小单位。 `页`的本质就是一块`16KB`大小的存储空间，`InnoDB`为了不同的目的而把`页`分为不同的类型，其中用于存放记录的页也称为`数据页`，我们先看看这个用于存放记录的页长什么样。数据页代表的这块`16KB`大小的存储空间可以被划分为多个部分，不同部分有不同的功能。

|         名称         |       中文名       | 占用空间大小 |       简单描述       |
| :------------------: | :----------------: | :----------: | :------------------: |
|    `File Header`     |       文件头       |   `38`字节   |   一些描述页的信息   |
|    `Page Header`     |        页头        |   `56`字节   |     页的状态信息     |
| `Infimum + Supremum` | 最小记录和最大记录 |   `26`字节   |   两个虚拟的行记录   |
|    `User Records`    |      用户记录      |    不确定    | 实际存储的行记录内容 |
|     `Free Space`     |      空闲空间      |    不确定    |  页中尚未使用的空间  |
|   `Page Directory`   |       页目录       |    不确定    |  页中的记录相对位置  |
|    `File Trailer`    |      文件结尾      |   `8`字节    |    校验页是否完整    |

InnoDB存储的基本结构是页，各个数据页有都头尾指针，可以组成一个双向链表，而每个数据页的用户记录又组成了一个单向链表。每个数据页都会为存储里面的记录生成一个页目录，再通过主键查找记录的时候可以在页目录中使用二分法快速定位到对应的槽，遍历该槽对应的分组记录即可快速找到指定记录。而以非主键作为搜索条件只能从最小记录开始一次遍历单链表的每条记录。

有两种类型索引：聚簇索引和非聚簇索引。对InnoDB来说，聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它将数据行保存在索引的叶子页中，聚簇的意思就是数据行和相邻的键值紧凑地存储在一起。因为数据无法同时存储在两个地方，所以一个表只能由一个聚簇索引。

​		聚簇索引的每一个叶子节点都包含了主键值，事务ID，用于事务和MVCC的回滚指针以及所有的剩余列。

​        InnoDB通过主键聚集数据，如果没有主键，InnoDB会选择一个唯一的非空索引代替，如果没有，InnoDB会隐式定义一个主键来作为聚簇索引。聚簇索引的叶子节点主要包含索引的值和行指针，而非聚簇索引的叶子节点存储的是主键值，并以此作为指向行的“指针”，这样的策略减少了出现行移动或者数据页分裂时二级索引的维护工作，在移动行时无须更新二级索引中的这个指针。

​		聚簇的数据有很多优点：

- 可以把相关数据保存在一起，例如实现电子邮箱，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数数据页就能获取某个用户的全部邮件。
- 数据访问更快，聚簇索引将索引和数据保存在同一个B树中，因此从聚簇索引中获取数据比非聚簇索引快。
- 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。

同时也会带来一些缺点：

- 聚簇数据最大限度地提高了I/O密集型应用地性能，但是如果数据全部放入到内存中，聚簇索引就没有优势。
- 更新聚簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置。
- 基于聚簇索引的表插入新行，或者更新需要导致移动行的时候，可能会面临页分裂的问题，当行的主键值要求必须将这一行插入到某个已满的页中，存储引擎会将该页分裂成两个页面容纳该行。
- 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候。
- 非聚簇索引的叶子节点由于包含了引用行的主键列，可能比想象的要更大。
- 非聚簇索引访问需要查询两次索引。

​        在数据库中，顺序读是指根据索引的叶节点就能顺序地读取所需地行数据。这个顺序只是逻辑上地顺序，在物理磁盘上可能还是随机读取。但相对来说，物理磁盘的数据还是相对比较顺序的。随机读，一般是指访问辅助索引叶节点不能完全得到结果，需要根据辅助索引叶节点中的主键去找实际行数据。因为一般辅助索引和主键所在的主键不同，访问是随机的方式。所以覆盖索引由于不需要进行第二次的随机读，可以大大优化查询速度。此外，高选择性的列可以更少的进行二次查询，也可以提高查询速度。

可以使用explain命令来分析SQL语句的执行计划。

exlain给出的列有id、select_type、table、type、possible_keys、key、key_len、ref、rows、Extra

id是select识别符，是select查询序列号。

**select_type**是select类型，有8种类型（说几个常用的SIMPLE、PRIMARY、UNION、SUBQUERY就行）

(1) SIMPLE(简单SELECT，不使用UNION或子查询等)

(2) PRIMARY(子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY)

(3) UNION(UNION中的第二个或后面的SELECT语句)

(4) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询)

(5) UNION RESULT(UNION的结果，union语句中第二个select开始后面所有select)

(6) SUBQUERY(子查询中的第一个SELECT，结果不依赖于外部查询)

(7) DEPENDENT SUBQUERY(子查询中的第一个SELECT，依赖于外部查询)

(8) DERIVED(派生表的SELECT, FROM子句的子查询)

(9) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)

**table**表示这一步访问的数据库表名

**type**：指的是对表访问方式，表示MySQL在表中找到所需行的方式

常用的有ALL、index、range、ref、eq_ref、const、system、NULL(性能由差到好)

ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行

index: Full Index Scan，index类型只遍历索引树

range:只检索给定范围的行，使用一个索引来选择行

ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值

eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件。

const、system: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system。

NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。

**possible_keys** 

指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用（该查询可以利用的索引，如果没有任何索引显示 null）
**Key**

key列显示MySQL实际决定使用的键（索引），必然包含在possible_keys中

**key_len**

表示索引中使用的字节数

**ref**

列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值

**rows**

估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数

**Extra**

**该列包含MySQL解决查询的详细信息,有以下几种情况：**

Using where:不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤

Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by ; order by

Using filesort：当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序”

Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。

Impossible where：这个值强调了where语句会导致没有符合条件的行（通过收集统计信息不可能存在结果）。

Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行

No tables used：Query语句中使用from dual 或不含任何from子句

# Redis数据结构

redis的数据结构主要有字符串string，链表list，哈希表hash，集合set，有序集合zset

## string实现：SDS

redis没有直接使用C语言传统字符串string,而是自己构建了一种名为简单动态字符串的数据结构，简称SDS。

SDS主要用来保存数据库的字符串值，还备用做AOF模块的缓冲区。

SDS结构有三部分组成，

1.buf数组用来保存字符串，

2.len等于SDS所保存字符串的长度，

3.free记录buf数组种未使用字节的数量

相对于 C 语言对于字符串的定义，多出了 len 属性以及 free 属性。好处是：

　　**①、可以用常数复杂度来获取字符串长度**

　　由于 len 属性的存在，我们获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)。而对于 C 语言，获取字符串的长度通常是经过遍历计数来实现的，时间复杂度为 O(n)。

　　**②、不会发生缓冲区溢出**

　　 C 语言中使用 strcat 函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。对于 SDS 数据类型，在进行字符修改的时候，会首先根据记录的 len 属性检查内存空间是否满足需求，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。

　　**③、可以减少修改字符串的内存重新分配次数**

　　C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。

　　而对于SDS，由于len属性和free属性的存在，对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略：

　　1、空间预分配：空间预分配用于优化对字符串进行空间扩展的操作，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。（扩展后len<1MB，多分配len长度空间，>1MB,多分配1MB，都会多分配一字节保留最后的空字符串）

　　2、惰性空间释放：惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的 字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录 来，并等待将来使用。

　　**④、保证了二进制安全**

　　因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件，内容可能包括空字符串，因此C字符串无法正确存取；而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素，数据在写入时是什么样的，它被读取就是什么样。并且 SDS 不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。

　　**⑤、兼容部分 C 字符串函数**

　　虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库<string.h> 中的一部分函数。

## list实现：双向链表

listNode由前置节点，后置节点和节点值组成。

list结构为链表提供了表头指针head,表尾指针tail，以及链表长度计数器len。此外还包含节点值复制函数dup、节点值释放函数free、节点值对比函数match三个成员函数。

redis链表特性有：双端双向无环，带有链表长度计数器。并且链表节点使用void*指针来保存节点值，所以可以用于保存不同类型的值。

在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是 压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。如果太大会将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。

Redis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。 

## 字典实现：哈希表

Redis的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。

```c
struct dictEntry {
 void* key;
 void* val; 
    /*    union {
        void *val;
        uint64_t u64;
        int64_t s64;
        double d;
    } v;*/
    
 dictEntry* next; // 链接下一个 entry
}
struct dictht {
 dictEntry** table; // 二维
 long size; // 第一维数组的长度
 long used; // hash 表中的元素个数
 ...
}
struct dict {
 ...
 int rehashidx;
 dictht ht[2];
}
```

哈希表dicth是由哈希表数组table，哈希表大小size， 用于计算索引值的大小掩码sizemask，，已有节点的数量used组成。

table是一个二维数组，数组的每个元素都是指向哈希表节点dictEntry结构的指针，每个dictEntry结构都保存这一个键值对key和value。以及指向下个哈希表的节点next。

key 是任意指针类型，而value可以是一个指针，或者是一个uint64，或者是一个int64的整数。next将多个哈希值的键值对连接在一起，解决键冲突的问题。

当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面，我们称这些键发生了冲突Redis的哈希表使用拉链发来解决冲突,每个哈希表节点都有一个next指针。多个哈希表节点可以用next指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表链接起来，新节点总是插在链表的表头

在redis字典dict中初始定义了两个哈希表dicth，一般情况 下字典只使用ht[0]哈希表，ht[1] 哈希表只会在对ht[0] 哈希表进行rehash时使用。另一个和rehash有关的属性是rehashindex，如果没有进行rehash，它的值为-1。

还有两个属性是type和privdata，为了创建多态字典而实现。type指向了dictType结构，，每个dictType结构保存了一簇用于操作特定类型键值对的函数。

向字典中添加数据redis做了这样几件事情:
 ① 判断 dict 是否正在进行rehash（即是否在扩容或收缩），如果有则多扩容或收缩一个步长。

​          随着不断进行操作，哈希表保存的键值会不断增加或减少，为了使负载因子保持再一个合理范围内，哈希表会进行相应的扩展或者收缩。即rehash操作。

​          哈希表rehash操作会先为字典的ht[1]分配空间，如果执行的是扩展操作，那么ht[1]的大小等为第一个大于等于ht[0] .used*2^n。如果是收缩操作，大小等于第一个大于等于ht[0].used的2^n。n的大小等于传来的size。之后将保存在ht[0]中的所有键值对rehash到ht[1]上面。最后当ht[0]所有数据迁移到ht[1]之后释放ht[0]表，将ht[1]设置为ht[0]，并为ht[1]创建一个空白的新表。

​          但是如果哈希表有几百几千万个数据，一次性将这些键值对全部rehash到ht[1]的话，可能会使服务器在一段时间内停止服务。所以redis采用了分多次，渐进式地将ht[0]慢慢地rehash到ht[1]中。

​      1 ) 为 ht[1]分配空间，让字典同时持有ht[0]和 ht[1]两个哈希表。 

​      2 ) 在字典中维持一个索引计数器变量rehashidx,并将它的值设置为0 , 表示rehash工作正式开始。 

​      3 ) rehash进行期间，每次对字典执行添加、删除、査找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1] , rehash工作完成之后, 程序将redisidx属性的值增1。 

​      4 ) 随着字典操作的不断执行，最终在某个时间 上，ht[0]的所有键值对都会被 rehash至 ht [1] , 这时程序将rehashidx属性的值设为-1，表示rehash操作已完成。

 ② 计算 key 的hash 值，判断该值是否已经存在，如果存在则直接返回

​         程序会根据键值对的key计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点到哈希表数组的指定索引上面。 哈希值是通过type指向的hashFunction实现的，redis使用的是MurmurHash2算法，索引值是通过哈希值与掩码sizemask相与来计算。如果索引值相同且元素一样就返回，如果元素不一样，使用头插法解决冲突问题。

 ③ 如果正在rehash，则往 ht[1] 里面添加数据，否则往 ht[0] 里面添加数据
 ④ 计算索引值，根据索引值找到ht[0]或者ht[1]的位置，分配内存，如果每个hash节点维护着节点链表，则用头插法插入节点



**set实现：intset**：元素不多时且集合只包含整数值元素使用，其余时候用哈希表



## zset实现：哈希表+跳跃表

跳跃表是一种有序的数据结构，它通过每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。跳跃表查找节点平均复杂度O（logN），最差复杂度O（N）。还可以通过顺序性操作来批量处理节点。在大部分情况下，跳跃表的效率可以和平衡树相媲美，而且实现要更为简单。

跳表具有如下性质：
 (1) 由很多层结构组成
 (2) 每一层都是一个有序的链表
 (3) 最底层(Level 0)的链表包含所有元素
 (4) 如果一个元素出现在 Level i 的链表中，则它在 Level i 之下的链表也都会出现。
 (5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素。

```c
struct zslnode { 
 string value; 
 double score; 
 zslnode*[] forwards; // 多层连接指针 
 zslnode* backward; // 回溯指针 
} 

struct zsl { 
 zslnode* header; // 跳跃列表头指针 
 int maxLevel; // 跳跃列表当前的最高层 
 map<string, zslnode*> ht; // hash 结构的所有键值对 
}
```

redis的跳跃表由zskiplistNode和zskiplist两个结构定义，其中zskiplistNode结构用来表示跳跃表节点，而zskiplist用来保存跳跃表节点相关信息。

header,指向跳跃表的表头节点，tail指向跳跃表的表尾节点。level记录目前跳跃表内层数最大的哪个节点的层数（表头层数不记录）。length记录跳跃表的长度（表头不算）

zskiplistNode包含层level，每个层都带有两个属性，前进指针和跨度，前进指针用于访问位于表尾方向的其他节点，而跨度记录了前进指针节点与当前节点的距离。

后退指针，指向当前节点的前一个节点，用于从后往前遍历。

分值，是一个double类型的浮点数，跳跃表的所有节点都是按分值从小到大来排序。

成员对象obj是一个指针，指向一个保存SDS值的字符串对象。

​         插入过程首先要在搜索合适插入点的过程中将「搜索路径」摸出来了，然后开始创建新节点，创建的时候需要给这个节点随机分配一个层数，redis每一层的晋升概率是 50%，再将搜索路径上的节点和这个新节点通过前向后向指针串起来。如果分配的新节点的高度高于当前跳跃列表的最大高度，就需要更新一下跳跃列表的最大高度。

​        redis跳跃表总共有64层，但是因为层数一般不高，所以遍历的时候从顶层开始往下遍历会非常浪费。跳跃列表会记录一下当前的最高层数 maxLevel，遍历时从这个 maxLevel 开始遍历性能就会提高很多。

## 压缩列表：hash和zset元素少的时候使用

压缩列表是redis为了节约内存开发，是由一系列特殊编码的连续内存块组成的顺序型数据结构。一个压缩列表可以包含任意多个节点，每个节点可以保存一个字节数组或者一个整数值。

 int  prevlen; // 前一个 entry 的字节长度 

 int  encoding; // 元素类型编码 

 optional byte[] content; // 元素内容

它的 prevlen 字段表示前一个 entry 的字节长度，当压缩列表倒着遍历时，需要通过这个字段来快速定位到下一个元素的位置。encoding 字段存储了元素内容的编码类型信息，ziplist 通过这个字段来决定后面的content 内容的形式。

因为 ziplist 都是紧凑存储，没有冗余空间 (对比一下 Redis 的字符串结构)。意味着插入一个新的元素就需要调用 realloc 扩展内存。如果 ziplist 占据内存太大，重新分配内存和拷贝内存就会有很大的消耗。所以 ziplist 不适合存储大型字符串，存储的元素也不宜过多。

# Redis其他

## 关系型数据库和非关系型数据库比较

非关系型数据库的优势：

性能
NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。

可扩展性
同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。

关系型数据库的优势：

复杂查询
可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。

事务支持
使得对于安全性能很高的数据访问要求得以实现。

## redis单线程模型

 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190918215924363.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hwX3hweHA=,size_16,color_FFFFFF,t_70) 

(为什么快：内存操作，多路I/O，单线程避免I/O）

(reactor模型，多路复用I/O epoll后面讲)

**文件事件处理器**
redis基于**reactor模式开发了网络事件处理器**，叫做**文件事件处理器**。**这个文件事件处理器，是单线程的，redis才叫做单线程的模型，这个处理器采用IO多路复用机制同时监听多个socket，根据socket上的事件来选择对应的事件处理器来处理这个事件。**

如果被监听的socket准备好执行accept、read、write、close等操作的时候，跟操作对应的文件事件就会产生，这个时候文件事件处理器就会调用之前关联好的事件处理器来处理这个事件。

**文件事件处理器是单线程模式运行的，但是通过IO多路复用机制监听多个socket，可以实现高性能的网络通信模型，又可以跟内部其他单线程的模块进行对接，保证了redis内部的线程模型的简单性。**

**文件事件处理器**的结构包含4个部分：**多个socket，IO多路复用程序，文件事件分派器，事件处理器**（命令请求处理器、命令回复处理器、连接应答处理器，等等）。

多个socket可能并发的产生不同的操作，每个操作对应不同的文件事件，但是IO多路复用程序会监听多socket，但是会将socket放入一个队列中排队，每次从队列中取出一个socket给事件分派器，事件分派器把socket给对应的事件处理器。

然后一个socket的事件处理完之后，IO多路复用程序才会将队列中的下一个socket给事件分派器。文件事件分派器会根据每个socket当前产生的事件，来选择对应的事件处理器来处理。

**文件事件**

当socket变得可读时（比如客户端对redis执行write操作，或者close操作），或者有新的可以应答的socket出现时（客户端对redis执行connect操作），socket就会产生一个AE_READABLE事件。

当socket变得可写的时候（客户端对redis执行read操作），socket会产生一个AE_WRITABLE事件。

IO多路复用程序可以同时监听AE_REABLE和AE_WRITABLE两种事件，要是一个socket同时产生了AE_READABLE和AE_WRITABLE两种事件，那么文件事件分派器优先处理AE_REABLE事件，然后才是AE_WRITABLE事件。

Redis 会将每个客户端套接字都关联一个指令队列。客户端的指令通过队列来排队进行顺序处理，先到先服务。

Redis 也会为每个客户端套接字关联一个响应队列。Redis 服务器通过响应队列来将指令的返回结果回复给客户端。

**文件事件处理器**

如果是客户端要连接redis，那么会为socket关联连接应答处理器，如果是要写数据，那么会关联命令请求处理器，如果是要读数据，那么会关联命令回复处理器。

**redis定时任务处理**

Redis 的定时任务会记录在一个称为最小堆的数据结构中。这个堆中，最快要执行的任务排在堆的最上方。在每个循环周期，Redis 都会将最小堆里面已经到点的任务立即进行处理。

## redis持久化

Redis 的持久化机制有两种，第一种是RDB，第二种是 AOF 日志。RDB是一次全量备份，AOF 日志是连续的增量备份。RDB是内存数据的二进制序列化形式，在存储上非常紧凑，而 AOF 日志记录的是内存数据修改的指令记录文本。AOF 日志在长期的运行过程中会变的无比庞大，数据库重启时需要加载 AOF 日志进行指令重放，这个时间就会无比漫长。所以需要定期进行 AOF 重写。

RDB

Redis 在持久化时会调用 库 的函数 fork 产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端请求。子进程做数据持久化，它不会修改现有的内存数据结构，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是父进程不一样，它必须持续服务客户端请求，然后对内存数据结构进行不间断的修改。 

这个时候就会使用操作系统的 COW 机制（写时复制）来进行数据段页面的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份分离出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬间的数据。

AOF

AOF 日志存储的是 Redis 服务器的顺序指令序列，AOF 日志只记录对内存进行修改的指令。

AOF持久化功能的实现可以分为命令追加（append)、文件写入、文件同步（sync) 三个步骤。

当AOF持久化功能处于打开状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾，Redis的服务器进程就是一个事件循环，在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile这个函数数，考虑是否需要将aof_buf缓冲区中的内容写人和保存到AOF 文件里面。

随着写入不断增多，aof日志会不断增大，Redis还提供了 bgrewriteaof 指令用于对 AOF日志进行重写。其原理是开辟一个子进程对内存进行遍历转换成一系列 Redis 的操作指令（只关注当前内存的数值可以怎么产生，不关心过去的过程），序列化到一个新的 AOF 日志文件中。序列化完毕后再将操作期间发生的增量 AOF 日志追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件。

因为子进程在进行AOF 重写期间，服务器 进程还需要继续处理命令请求，而新的命令可能会对现有的数据库状态进行修改，从而使得服务器当前的数据库状态和重写后的AOF文件所保存的数据库状态不一致。 为了解决这种数据不一致问题，Redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区

## redis淘汰删除策略

redis会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会遍历这个字典来删除到期的key。

·定时删除：在设置键的过期时间的同时，创建一个定时器（timer），让定时器在键的过期时间来临时，立即执行对键的删除操作。 对内存是友好的，但是对cpu不友好过期键比较多的话删除行为就会占用cpu性能 。

·惰性删除：放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。 对CPU最友好、只有查出键时才检查是否过期，但是对内存不够友好，会占用大量内存不能及时释放。
·定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。



当Redis的内存使用达到设置的内存上限，触发内存淘汰机制，根据淘汰规则释放内存：

　　1.volatile-lru：使用lru算法（Least Recently Used,最近最久未使用），从已设置过期时间的数据集中挑选最近最少使用的淘汰；

　　2.volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰；

　　3.volatile-random：从已设置过期时间的数据集中任意选择数据淘汰；

　　4.allkeys-lru：使用lru算法，从数据集中选择最近最少使用的淘汰；

　　5.allkeys-random：从数据集中选择任意数据淘汰；

　　6.noenviction(驱逐)：禁止淘汰数据；当内存不足以写入新数据时，直接报异常，Redis只响应读操作；

LRU算法实现：

 LRU 缓存算法的核心数据结构就是哈希链表，双向链表和哈希表的结合体。 就是借助哈希表赋予了链表快速查找的特性：可以快速查找某个 key 是否存在缓存（链表）中，同时可以快速删除、添加节点。 

```java
class LRUCache {
    // key -> Node(key, val)
    private HashMap<Integer, Node> map;
    // Node(k1, v1) <-> Node(k2, v2)...
    private DoubleList cache;
    // 最大容量
    private int cap;
    
    public LRUCache(int capacity) {
        this.cap = capacity;
        map = new HashMap<>();
        cache = new DoubleList();
    }
    
    public int get(int key) {
        if (!map.containsKey(key))
            return -1;
        int val = map.get(key).val;
        // 利用 put 方法把该数据提前
        put(key, val);
        return val;
    }
    
    public void put(int key, int val) {
        // 先把新节点 x 做出来
        Node x = new Node(key, val);
        
        if (map.containsKey(key)) {
            // 删除旧的节点，新的插到头部
            cache.remove(map.get(key));
            cache.addFirst(x);
            // 更新 map 中对应的数据
            map.put(key, x);
        } else {
            if (cap == cache.size()) {
                // 删除链表最后一个数据
                Node last = cache.removeLast();
                map.remove(last.key);
            }
            // 直接添加到头部
            cache.addFirst(x);
            map.put(key, x);
        }
    }
}

```



Redis 使用的是一种近似 LRU 算法，它跟 LRU 算法还不太一样。之所以不使用 LRU 算法，是因为需要消耗大量的额外的内存，需要对现有的数据结构进行较大的改造。在现有数据结构的基础上使用随机采样法来淘汰元素，能达到和 LRU 算法非常近似的效果。Redis 为实现近似 LRU 算法，它给每个 key 增加了一个额外的小字段，是最后一次被访问的时间戳。

LRU的处理方式只有懒惰处理。当 Redis 执行写操作时，如果发现内存超出 maxmemory，就会执行一次 LRU淘汰算法。redis近似的LRU就是随机采样出 5(可以配置) 个 key，然后淘汰掉最旧的 key，如果淘汰后内存还是超出 maxmemory，那就继续随机采样淘汰，直到内存低于maxmemory 为止。 

# JAVA Map

CopyOnWriteList

 我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 

借用了linux fork创建进程的思想

## HashMap

 HashMap 底层的数据结构主要是：数组 + 链表 + 红黑树。其中当链表的长度大于等于 8 时，链表会转化成红黑树，当红黑树的大小小于等于 6 时，红黑树会转化成链表。

HashMap既继承了AbstractMap，又实现了Map的接口。但实际上AbstractMap已经实现了Map的接口。据说JDK维护的人觉得这是一个小失误，但是不值得去修改，就这样一直保留了下来。

HashMap在开头定义了几个初始参数，有初始容量16，在代码中是用1<<（左移）4来表示的，是为了提示使用者输入容量的参数都会变为2的幂的大小。最大容量1<<30，负载因子默认值0.75，数组阈值threshold，桶上的链表或者红黑树相互转化的大小8和6。记录HashMap结构是否发生变化的值modCount，以及hashmap实际的大小和存放数据的数组。



这个数组的数据类型是Node<K,V>，这个类型实现了Map.Entry接口，定义了int类型的hash值，key值，value值以及节点指针next。并实现了Map.Entry接口的getset方法，以及hashCode()和equals()的方法。这个hashCode是将key与value的哈希值进行异或运算得到的。equals方法也是只有在key**值**相同且value**值**（用的equal，不是==）相同才返回true。

TreeNode<K,V>继承了LinkedHashMap.Entry<K,V>而后者又继承了HashMap.Node<K,V>。所以TreeNode依然保有Node的属性，同时由于添加了prev这个前驱指针使得链表变为了双向的。 



之后就是HashMap的方法：

hash() 散列函数，这个函数得到了散列值，它首先判断了key是否存在，如果不存在就返回0，如果存在，返回key的hashCode与它16位右移的异或。这段代码叫做扰动函数。

为什么要用扰动函数：

 理论上散列值是一个int型，如果直接拿散列值作为下标访问HashMap主数组的话，考虑到2进制32位带符号的int表值范围从**-2147483648**到**2147483648**。前后加起来大概40亿的映射空间。只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。

但问题是一个40亿长度的数组，内存是放不下的。HashMap扩容之前的数组初始大小才16。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来访问数组下标。源码中模运算是在这个indexFor( )函数里完成的。  indexFor就是把散列值和（数组长度-1）做一个"与"操作， 

 这也正好解释了为什么HashMap的数组长度要取2的整数幂。因为这样（数组长度-1）正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。 

 所以扰动函数将hashCode右位移16位，正好是32bit的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。 



HashMap有四种构造函数：无参构造，初始容量和负载因子都是用默认值。传入初始容量的构造函数。

传初始容量和负载因子的构造函数，这个函数是前两个的实现，如果传入的初始容量大于最大容量，那么初始容量就等于最大容量，如果传入的是正确参数，那么HashMap的初始容量等于大于等于这个数的第一个2的幂。这个函数也很巧妙，先将输入的值减1，再对其进行或操作，保证了在输入2的幂时，比如16，返回值也是16而不是32。

最后一个是有一个Map类型的参数的构造方法，这个方法会调用了putMapEntries，会创建出一个负载因子位默认值，大小为传入的map大小，并且有和传入的map相同数据的一个新的map。



HashMap存储的键值对，用get(K)方法来获取V，用containsKey(K)方法来检查K是否存在。 而get与containsKey方法都是用getNode方法实现的。这个方法一般会传入key的哈希值和key值。通过哈希值的位运算计算出key节点在数组的位置。然后判断这个节点是树节点还是链表节点，然后分别调用对应的遍历函数进行查找。将查找到的Node节点返回。

HashMap 遍历使用的是一种快速失败机制，它是 Java 非安全集合中的一种普遍机制，这种机制可以让集合在遍历时，如果有线程对集合进行了修改、删除、增加操作，会触发并发修改异常。

它的实现机制是在遍历前保存一份 modCount ，在每次获取下一个要遍历的元素时会对比当前的 modCount 和保存的 modCount 是否相等。

快速失败也可以看作是一种**安全机制**，这样在多线程操作不安全的集合时，由于快速失败的机制，会抛出异常。



而put方法是使用putVal方法实现的。这个方法首先会判断Node节点的数组table有没有初始化，如果没有，调用resize方法进行初始化。如果根据哈希值计算出的索引值从数组获取的节点为空，那么直接创建节点插入到数组中。如果新插入的节点与table中节点的hash值相同（==）并且key相等（equal|| == ）直接将节点替换。如果不相等，先判断当前是链表结构还是红黑树。如果是红黑树，使用红黑树的插入操作。如果是链表，遍历这个链表，如果遇到key相同的节点，替换掉。如果遍历到链表的节位，说明是新的节点，将节点插入到链表末尾。此时需要判断一下链表长度是否>=8，如果满足条件，调用treeifyBin，将链表转化为红黑树。

putVal还有个onlyIfAbsent的参数， 作用是当要插入的key已存在时是否替换value，false则替换，true则不替换。 

最后判断size的大小是否大于阈值threshold，如果大于进行扩容。

resize扩容方法

如果HashMap的大小超过了负载因子(load factor)定义的容量就会扩容，默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。这个值只可能在两个地方，一个是原下标的位置，另一种是在下标为<原下标+原容量>的位置

扩容时会先创建一个oldTab等于现在的table，判断现在的table是否为空，容量是否为0，如果容量为0那么用默认构造器先对其进行创建。否则，创建出一个新表，这个表的容量是旧表的两倍。之后遍历原来table的数据放到扩容后的新表中。扩容时因为length长度发生变化，所计算的索引值也都变了。

treeifyBin：红黑树节点是双向的，树化的时候先将单向链表转化为双向链表，之后根据链表结构创建红黑树结构。



## LinkedHashMap

LinkedHashMap 本身是继承 HashMap 的，所以它拥有 HashMap 的所有特性，再此基础上，还提供了两大特性：

- 按照插入顺序进行访问；
- 实现了访问最少最先删除功能，其目的是把很久都没有访问的 key 自动删除。

  ![这里写图片描述](https://img-blog.csdn.net/20170512160734275?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvanVzdGxvdmV5b3Vf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

LinkedHashMap 的数据结构几乎就是把 LinkedList 的元素换成了 HashMap 的 Node，像是两者的结合体，因为增加了这些结构,把 Map 的元素都串联起来，形成一个链表，就可以维护元素插入进来的顺序。 它重新定义了数组中保存的元素Entry，该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而在哈希表的基础上又构成了双向链接列表。 

 LinkedHashMap比HashMap多了三个属性: 链表的头节点，尾节点和 取得键值对的方式accessOrder,是个布尔值,false表示插入顺序,true表示访问顺序。根据访问顺序可以用它来实现LRU。

LinkedHashMap只重写了get操作，但是没有重写put和remove操作。

在HashMap中定义了三个LinkedHashMap使用的空方法，并在LinkedHashMap重写。分别是`afterNodeRemoval`、`afterNodeInsertion`、`afterNodeAccess` 在HashMap的put操作中使用了`afterNodeInsertion`和`afterNodeAccess` ，由于多态的机制，在LinkedHashMap对象调用put的时候会使用子类的实现。

   `afterNodeRemoval`，是当删除了某个节点后需要做的操作。 

   `afterNodeInsertion`，是当插入了某个节点后需要做的操作。 但 *LinkedHashMap中的removeEldestEntry方法永远返回false* ，如果需要实现LRU，需要重写这个方法。return size() > capacity;

afterNodeAccess，这个方法是访问了某个key-value后的操作（还记得我们之前特意提到的属性accessOrder吗？不记得往前翻翻）。当accessOrder == true时，每当我们调用get方法，都会将查找到的key-value移动到链表的尾端。

## TreeMap

​            TreeMap容器的主要作用是按照插入的key进行排序（默认升序）。TreeMap容器维护一棵红黑树，只要比较key就可以进行查找元素、确定插入的位置。

TreeMap 底层的数据结构就是红黑树，和 HashMap 的红黑树结构一样。

因为底层使用的是平衡红黑树的结构，所以 containsKey、get、put、remove 等方法的时间复杂度都是 log(n)。

TreeMap内部类`Entry`对象是`TreeMap`中的节点，用来存放一个`key-value`，以及维护红黑树节点之间的关系，主要属性: 	 key;value; 左子节点、右子节点、父节点、节点的颜色color 默认是 BLACK;
之后大都是红黑树的操作。

## ConcurrentHashMap

**1.注释**

1. 所有的操作都是线程安全的，我们在使用时，无需再加锁；
2. 多个线程同时进行 put、remove 等操作时并不会阻塞，可以同时进行，和 HashTable 不同，HashTable 在操作时，会锁住整个 Map；
3. 迭代过程中，即使 Map 结构被修改，也不会抛 ConcurrentModificationException 异常；
4. 除了数组 + 链表 + 红黑树的基本结构外，新增了转移节点，是为了保证扩容时的线程安全的节点；
5. 提供了很多 Stream 流式方法，比如说：forEach、search、reduce 等等。

**2.与HashMap结构的不同点：**

1. 红黑树结构略有不同，HashMap 的红黑树中的节点叫做 TreeNode，TreeNode 不仅仅有属性，还维护着红黑树的结构，比如说查找，新增等等；ConcurrentHashMap 中红黑树被拆分成两块，TreeNode 仅仅维护的属性和查找功能，新增了 TreeBin，来维护红黑树结构，并负责根节点的加锁和解锁；
2. 新增 ForwardingNode （转移）节点，扩容的时候会使用到，通过使用该节点，来保证扩容时的线程安全。

**3.put操作**(如何保证线程安全)

ConcurrentHashMap 在 put 方法上的整体思路和 HashMap 相同，但在线程安全方面写了很多保障的代码，我们先来看下大体思路：

1. 如果数组为空，初始化。
2. 计算当前槽点有没有值，没有值的话，利用CAS创建，失败继续自旋（for 死循环），直到成功，
3. 槽点有值的话，如果槽点是转移节点(正在扩容)，就会一直自旋等待扩容完成之后再新增，不是转移节点
4. 槽点有值的，先锁定当前槽点，保证其余线程不能操作，如果是链表，新增值到链表的尾部，如果是红黑树，使用红黑树新增的方法新增；
5. 新增完成之后 check 需不需要扩容，需要的话去扩容。

**3.1 数组初始化时的线程安全**

数组初始化时，首先通过自旋来保证一定可以初始化成功，然后通过 CAS 设置 SIZECTL 变量的值，来保证同一时刻只能有一个线程对数组进行初始化，CAS 成功之后，还会再次判断当前数组是否已经初始化完成，如果已经初始化完成，就不会再次初始化，通过自旋 + CAS 双重检测 等手段保证了数组初始化时的线程安全。

**3.2 新增槽点值时的线程安全**

此时为了保证线程安全，做了四处优化：

1. 通过自旋死循环保证一定可以新增成功。

在新增之前，通过 `for (Node[] tab = table;;)` 这样的死循环来保证新增一定可以成功，一旦新增成功，就可以退出当前死循环，新增失败的话，会重复新增的步骤，直到新增成功为止。

2. 当前槽点为空时，通过 CAS 新增。

Java 这里的写法非常严谨，没有在判断槽点为空的情况下直接赋值，因为在判断槽点为空和赋值的瞬间，很有可能槽点已经被其他线程赋值了，所以我们采用 CAS 算法，能够保证槽点为空的情况下赋值成功，如果恰好槽点已经被其他线程赋值，当前 CAS 操作失败，会再次执行 for 自旋，再走槽点有值的 put 流程，这里就是自旋 + CAS 的结合。

3. 当前槽点有值，锁住当前槽点。

put 时，如果当前槽点有值，就是 key 的 hash 冲突的情况，此时槽点上可能是链表或红黑树，我们通过用sychronized锁住槽点，来保证同一时刻只会有一个线程能对槽点进行修改。

4. 红黑树旋转时，锁住红黑树的根节点，保证同一时刻，当前红黑树只能被一个线程旋转。这个锁是用CAS实现的乐观锁，当LOCKSTATE状态为0，就写入1，表示占用锁，如果锁定失败就一直竞争。

**4.扩容中的关键点**

1. 首先需要把老数组的值全部拷贝到扩容之后的新数组上，先从数组的队尾开始拷贝；
2. 拷贝数组的槽点时，先把原数组槽点锁住，保证原数组槽点不能操作，成功拷贝到新数组时，把原数组槽点赋值为转移节点；
3. 这时如果有新数据正好需要 put 到此槽点时，发现槽点为转移节点，就会一直等待，所以在扩容完成之前，该槽点对应的数据是不会发生变化的；
4. 从数组的尾部拷贝到头部，每拷贝成功一次，就把原数组中的节点设置成转移节点；
5. 直到所有数组数据都拷贝到新数组时，直接把新数组整个赋值给数组容器，拷贝完成。
   

# Spring IoC

 **IoC，对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系。** 有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合的。

所有的类都会在spring容器中登记，告诉spring你是个什么东西，你需要什么东西，然后spring会在系统运行到适当的时候，把你要的东西主动给你，同时也把你交给其他需要你的东西。所有的类的创建、销毁都由 spring来控制，也就是说控制对象生存周期的不再是引用它的对象，而是spring。

IoC有三种注入方式，分别是setter方法注入，构造方法注入，接口注入。接口注入现在基本不用。

构造方法注入的优点是对象在构造完成之后，即已进入就绪状态，可以 马上使用。缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。setter方法刚好相反，描述性要好， 可以被继承，但是不能在构造完成后就进入就绪态。

spring有两个IoC容器

BeanFactory。基础类型IoC容器，提供完整的IoC服务支持。如果没有特殊指定，默认采用延迟初始化策略（lazy-load）。只有当客户端对象需要访问容器中的某个受管对象的时候，才对该受管对象进行初始化以及依赖注入操作。

ApplicationContext。ApplicationContext在BeanFactory的基础上构建，是相对比较高级的容器实现，除了拥有BeanFactory的所有支持，ApplicationContext还提供了其他高级特性，比如事件发布、国际化信息支持等。



使用IoC容器的第一步是注册对象，在Spring中这些注册的对象又被称为Bean。

在`BeanFactory`容器中，每一个注入对象都对应一个`BeanDefinition`实例对象，该实例对象负责保存注入对象的所有必要信息，包括其对应的对象的class类型、是否是抽象类、构造方法参数以及其他属性等。当客户端向`BeanFactory`请求相应对象的时候，`BeanFactory`会通过这些信息为客户端返回一个完备可用的对象实例。

而`BeanDefinition`实例对象的信息是从`BeanDefinitionReader取得的`，对应到`xml`配置文件，就是他的子类`XmlBeanDefinitionReader`，`XmlBeanDefinitionReader`负责读取`Spring`指定格式的`XML`配置文件并解析，之后将解析后的文件内容映射到相应的`BeanDefinition`。



**Bean的生命周期**

 ![img](https://upload-images.jianshu.io/upload_images/4476195-86fd1ae1d5ba92a7?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp) 

 当我们完成了容器的启动阶段后，对于`BeanFactory`来说，并不会马上实例化相应的`bean`定义。 容器现在仅仅拥有所有对象的`BeanDefinition`来保存实例化阶段将要用的必要信息。只有当请求方通过`BeanFactory`的`getBean()`方法来请求某个对象实例的时候，才有可能触发`Bean`实例化阶段的活动`BeanFactory`的`getBean()`法可以被客户端对象显式调用。 也可以在容器内部隐式地被调用。隐式调用有如下两种情况： 

- 对于`BeanFactory`来说，对象实例化默认采用延迟初始化。通常情况下，当`对象A`被请求而需要第一次实例化的时候，如果它所依赖的`对象B`之前同样没有被实例化，那么容器会先实例化`对象A`所依赖的对象。

-  `ApplicationContext`启动之后会实例化所有的`bean`定义，`ApplicationContext`在实现的过程中依然遵循`Spring`容器实现流程的两个阶段，只不过它会在启动阶段的活动完成之后，紧接着调用注册到该容器的所有`bean`定义的实例化方法`getBean()`。

Bean有5中作用域：

singleton:在Spring IoC容器中仅存在一个Bean实例，Bean以单例的方式存在，是容器的默认值

prototype:每次从容器调用Bean时，都会返回一个新的实例。

request:每次HTTP请求都会创建一个新的Bean

session:同一个HTTP Session共享一个Bean

globalsession,仅适用于WebApplicationContext环境

如果使用singleton，会产生**循环依赖**的问题

   Spring的单例对象的初始化主要分为三步： 


    ①：createBeanInstance：实例化，其实也就是 调用对象的构造方法实例化对象
    
    ②：populateBean：填充属性，这一步主要是多bean的依赖属性进行填充
    
    ③：initializeBean：调用spring xml中的init() 方法。
Spring使用了三级缓存解决了循环依赖的问题。在populateBean()给属性赋值阶段里面Spring会解析你的属性，并且赋值，当发现，A对象里面依赖了B，此时又会走getBean方法，但这个时候，你去缓存中是可以拿的到的。因为我们在对createBeanInstance对象创建完成以后已经放入了缓存当中，所以创建B的时候发现依赖A，直接就从缓存中去拿，此时B创建完，A也创建完，一共执行了4次。至此Bean的创建完成，最后将创建好的Bean放入单例缓存池中。

 接下来，容器会检查当前对象实例是否实现了一系列的以`Aware`命名结尾的接口定义。如果是，则将这些`Aware`接口定义中规定的依赖注入给当前对象实例。我们可以看一看这些`Aware`对象到底规定了什么依赖 

然后调用我们实现的Bean的后置处理器，给我们最后一次机会去修改Bean的属性。之后如果定义了 `init-method`，指定我们在容器中的获得的对象在执行任何方法前，先执行那个方法。 Bean就创建成功了。他们将一直驻留在应用上下文中，知道应用上下文被销毁。

# Spring AOP

AOP意为：面向切面编程， 可以通过[预编译](https://baike.baidu.com/item/预编译)方式和运行期动态代理实现在不修改[源代码](https://baike.baidu.com/item/源代码)的情况下给程序动态统一添加功能的一种技术。 

spring中可以使用动态代理和CGLIB实现， 如果目标对象是接口，则使用 JDK 动态代理，否则使用 CGLIB 来生成代理类 。

 动态代理其实就是`Java`中的一个方法，这个方法可以实现：**动态创建一组指定的接口的实现对象** 

动态代理步骤：
1.创建一个实现接口InvocationHandler的类，它必须实现invoke方法
2.创建被代理的类以及接口
3.通过Proxy的静态方法newProxyInstance创建一个代理
4.通过代理调用方法

# Spring的的事务传播机制

1. REQUIRED（默认）：支持使用当前事务，如果当前事务不存在，创建一个新事务。
2. SUPPORTS：支持使用当前事务，如果当前事务不存在，则不使用事务。
3. MANDATORY：强制，支持使用当前事务，如果当前事务不存在，则抛出Exception。
4. REQUIRES_NEW：创建一个新事务，如果当前事务存在，把当前事务挂起。
5. NOT_SUPPORTED：无事务执行，如果当前事务存在，把当前事务挂起。
6. NEVER：无事务执行，如果当前有事务则抛出Exception。
7. NESTED：嵌套事务，如果当前事务存在，那么在嵌套的事务中执行。如果当前事务不存在，则表现跟REQUIRED一样。



在TransactionDefinition接口中定义了五个不同的事务隔离级别

ISOLATION_DEFAULT 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.另外四个与JDBC的隔离级别相对应 
ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它充许别外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读

ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。

ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生(不可重复读)。

ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。

# Spring的生命周期

一、执行 AnnotationConfigApplicationContext的无参构造方法

二、.注册xml和注解的两种Reader类

三、1.prepareRefresh()      //刷新前的预处理

2.obtainFreshBeanFactory（）    //创建beanFactory   （DefaultListableBeanFactory）

3.prepareBeanFactory（beanFactory） 对beanFactory进行设置，例如设置类加载器、表达式解析器、environment beans

4.postProcessBeanFactory        beanFactory的后置工作，留给子类实现扩展

5.invokeBeanFactoryPostProcessor（beanFactory） 执行BeanFactoryPostProcessorde 的方法，先执行BeanDefinitionRegisterPostProcess可以对BeanDefinition就行修改扩展，然后再执行普通的BFPP

6.registerBeanPostProcessor    //创建BPP并添加到beanFactory中

7.initMessageSource（）  //初始化Message组件

8.initApplicationEventMuticaster（）  //初始化时间派发器（多播器）（有自己创建的优先用自己的，没有的话话系统自己创建）

9.onRefresh（） 留给子类容器扩展

10.registerListener（） 将容器中的监听器注册到多播器中

11.finishBeanFactoryInitialization（beanFactory）  初始化剩下的单例bean

12.finishRefresh（）  完成BeanFantory初始化的创建工作

# Bean的生命周期

1.实例化bean

2.调用populateBean方法进行初始化

3.各种Aware来获取BeanName、ClassLoader、beanFactory等

4.执行BeanPostProcessor 的beforeInitialization（例如@PostConstructor注解就是此时的CommonAnnotationBeanPostProcessor进行解析）

5.InitialBean  （实现了InitializingBean，实现afterPropertiesSet（）方法）

6.initMethod（）

7.执行BeanPostProcessor 的afterInitialization

8.使用

9.调用DisposableBean的destory（）

10.destoryMethod（）

# Spring、Spring MVC Spring Boot

# JVM垃圾回收

- 堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的目的就是存放对象实例。
- 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。
- 程序计数器（Program Counter Register）,是一块较小的内存空间，它的作用是当前线程所执行的字节码的行号指示器。
- JVM栈（JVM Stacks）是线程私有的。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。
- 本地方法栈（Native Method Stacks）与虚拟机栈很相似的，区别是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则是为虚拟机使用到的Native方法服务。

首先是判断哪些对象需要被回收

**引用计数法**

给每个对象添加一个计数器，当有地方引用该对象时计数器加1，当引用失效时计数器减1。用对象计数器是否为0来判断对象是否可被回收。缺点：**无法解决循环引用的问题**。

**可达性分析算法**

通过`GC ROOT`的对象作为搜索起始点，通过引用向下搜索，所走过的路径称为引用链。通过对象是否有到达引用链的路径来判断对象是否可被回收（可作为`GC ROOT`的对象：虚拟机栈中引用的对象，方法区中类静态属性引用的对象，方法区中常量引用的对象，本地方法栈中`JNI`引用的对象）

**Java有四种引用方式**：

 1、强引用
当我们使用new 这个关键字创建对象时被创建的对象就是强引用，如果一个对象具有强引用。垃圾回收器就不会去回收有强引用的对象。如当jvm内存不足时，具备强引用的对象，[虚拟机](http://www.2cto.com/os/xuniji/)宁可会报内存空间不足的异常来终止程序，也不会靠垃圾回收器去回收该对象来解决内存。
2、软引用
如果一个对象具备软引用，如果内存空间足够，那么垃圾回收器就不会回收它，如果内存空间不足了，就会回收该对象。当然没有被回收之前，该对象依然可以被程序调用。
3、弱引用
如果一个对象只具有弱引用，只要垃圾回收器在自己的内存空间中线程检测到了，就会立即被回收，对应内存也会被释放掉。相比软引用弱引用的生命周期要比软引用短很多。不过，如果垃圾回收器是一个优先级很低的线程，也不一定会很快就会释放掉软引用的内存。
4、虚引用
如果一个对象只具有虚引用，那么它就和没有任何引用一样，随时会被jvm当作垃圾进行回收。  只是回收时会有一个系统通知。

有三种常用的垃圾回收算法：

**标记-清除算法**

“标记-清除”（Mark-Sweep）算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。

它的主要不足有两个：

1. 效率问题，标记和清除两个过程的效率都不高；
2. 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。

**复制算法**

为了解决效率问题，“复制”算法出现了，它将可用内存按容量划分为两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况。

IBM 研究指出新生代中的对象 98% 是“朝生夕死”的，所以并不需要按照 1:1 的比例来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor 。

当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才用过的 Survivor 空间。HotSpot 虚拟机默认 Eden:Survivor = 8:1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（其中一块Survivor不可用），只有 10% 的内存会被“浪费”。

但我们没有办法保证每次回收都只有不多于10%的对象存活，当survivor空间不够用时，需要依赖老年代进行分配担保。

**标记-整理算法**

根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一段移动，然后直接清理掉端边界以外的内存。 



**HotSpot的算法实现**

1. 枚举根节点

在可达性分析中，要从 GC Roots 节点找引用链，可作为 GC Roots 的节点主要在全局性的引用与执行上下文中，如果要逐个检查这里面的引用，那么必然会消耗很多时间。这项分析工作必须不可以出现分析过程中对象引用关系还在不断变化的情况，否则分析结果准确性就无法得到保证。这样就导致 GC 进行时必须停顿所有 Java 执行线程（Sun将这件事情称为"Stop The World"），即使是在号称几乎不会发生停顿的 CMS 收集器中，枚举根节点时也是必须要停顿的。

因此，目前的主流 Java 虚拟机使用的都是准确式 GC，虚拟机可以知道内存中某个位置的数据具体是什么类型。

在 HotSpot 的实现中，是使用一组称为 OopMap 的数据结构来达到这个目的。在类加载完成的时候，HotSpot 就把对象内偏移量上是什么类型的数据计算出来，也会在特定的位置记录栈和寄存器中哪些位置是引用。 GC 在扫描时就会直接得知这些信息。

2. 安全点

有了 OopMap ，HotSpot 可以快速完成 GC Roots的 枚举，但是引用关系会有变化，如果为每一条指令都生成对应的 OopMap，那将会需要大量的额外空间。

所以OopMap只在“特定的位置”记录了这些信息，这些位置称为安全点，即程序执行时并非在所有地方都能停顿下来开始 GC ，只有在到达安全点时才能暂停。

对于在 GC 发生时让所有线程都“跑”到最近的安全点上再停顿下来，有两种方案可供选择：抢先式中断和主动式中断。

抢先式中断不需要线程的执行代码主动去配合，在 GC 发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。

主动式中断是当 GC 需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。轮询标志的地方和安全点是重合的，另外再加上创建对象需要分配内存的地方。

3. 安全区域

程序不执行的时候需要安全区域来解决线程回收位置的问题。

安全区域是指在一段代码片段之中，引用关系不会发生变化。

可以把 Safe Region 看做是被扩展了的 Safepoint。在线程执行到 Safe Region 中的代码时，首先标识自己已经进入了 Safe Region，当在这段时间里 JVM 要发起 GC 时，可以直接标识。在线程要离开 Safe Region 时，它要检查系统是否已经完成了根节点枚举（或者是整个 GC 过程），如果完成了，那线程就继续执行，否则它就必须等待直到收到可以安全离开 Safe Region 的信号为止。



**内存分配策略**

1 对象优先在Eden分配

大多数情况下，对象会在新生代`Eden`区中分配。当Eden区没有足够空间进行分配时，虚拟机会发起一次 `Minor GC`。Minor GC相比`Major GC`更频繁，回收速度也更快。通过Minor GC之后，Eden区中绝大部分对象会被回收，而那些存活对象，将会送到`Survivor`的From区（**若From区空间不够，则直接进入Old区**） 。

2 Survivor区

Survivor区相当于是Eden区和Old区的一个缓冲，Survivor又分为2个区，一个是From区，一个是To区。每次执行`Minor GC`，会将Eden区中存活的对象放到Survivor的From区，而在From区中，仍存活的对象会根据他们的年龄值来决定去向。**Survivor的存在意义就是减少被送到老年代的对象，进而减少`Major GC`的发生**。Survivor的预筛选保证，**只有经历16次`Minor GC`还能在新生代中存活的对象，才会被送到老年代**。

3 大对象直接进入老年代

所谓大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组。大对象对虚拟机的内存分配来说就是一个坏消息，经常出现大对象容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来 “安置” 它们。

虚拟机提供了一个`XX:PretenureSizeThreshold`参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制（新生代采用的是复制算法）。

4 长期存活的对象将进入老年代

虚拟机给每个对象定义了一个对象年龄（Age）计数器，如果对象在Eden出生并经过第一次`Minor GC`后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中（正常情况下对象会不断的在Survivor的From与To区之间移动），并且对象年龄设为1。对象在Survivor区中每经历一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认15岁），就将会晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数 `XX:MaxPretenuringThreshold` 设置。

5 动态对象年龄判定

为了能更好地适应不同程度的内存状况，虚拟机并不是永远地要求对象的年龄必须达到 `MaxPretenuringThreshold`才能晋升老年代，如果Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于改年龄的对象就可以直接进入老年代，无需等到`MaxPretenuringThreshold`中要求的年龄。



**垃圾收集器**

Serial收集器（串行收集器）

Serial 收集器是最基本的收集器，是一个单线程的收集器，它只会使用一条收集线程去完成垃圾收集工作，并且在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。使用复制算法。

Serial Old 收集器

Serial Old 是 Serial 收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。

ParNew收集器

ParNew 收集器其实就是 Serial 收集器的多线程版本。

Parallel Scavenge收集器

Parallel Scavenge 收集器是一个新生代收集器，也是使用复制算法，使用多个线程收集。 Parallel Scavenge 收集器的目标则是达到一个可控制的吞吐量 。 **（高效率的利用CPU）** 

Parallel Old收集器

**Parallel Scavenge收集器的老年代版本**。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。

CMS收集器

CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。

CMS收集器是一种 “标记-清除”算法实现的，但它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：

- 初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ；
- 并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。
- 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- 并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。

主要优点：**并发收集、低停顿**。缺点：

- **对CPU资源敏感；**
- **无法处理浮动垃圾；**
- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**



G1收集器

 **G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)**。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 

### **G1**    

+ 局部采用复制算法，但是整体使用标记-整理算法
+ 将整个堆划分成不同的Region，
+ 逻辑上不同的Region作为不同的区（老年代、新生代），但在物理上不区分  ***物理上不要求连续，逻辑上要求连续***
+ 每个Region没有固定的区，可能前后角色切换（新生代->老年代）
+ 每个区域大小在1M~32M，一般且最多设置2048个区域        ***也就是最大支持64G内存***

![1582427946284](最后的波纹.assets/1582427946284.png)

+ 回收步骤：***选的新Region（Survivor、Old）尽量得到连续***    小区域收集+形成连续的内存块
  + 多个Eden+部分Survivor ---->新的Survivor        
  + 部分Suvivor+If(Sorvivor空间不足，部分Eden)-------->Old

![1582428572919](最后的波纹.assets/1582428572919.png)

### **G1与CMS相比的优势**

+ 不会产生内存碎片
+ 可以精确控制停顿时间（***默认250ms***）
+ 适用于较大的堆

#### **G1内部细节**

+ 无需回收整个堆，而是选择一个Collection Set（CS）

+ 两种GC：

  + Fully young GC
  + old GC（包含了一次 fully young GC）
  + Mixed GC （筛选回收GC）

+ 估计每个Region中的垃圾比例，优先回收垃圾多的Region

+ 问题：

  + 跨代/Region引用

    <img src="面试相关/面试题/JVM.assets/1582546807774.png" alt="1582546807774" style="zoom:50%;" />

  + 引入了Card Table （将每个Region分成多个Card，Card Table 存储对应Card是否Dirty ***引用对象复制的时候会更新Card Table的值***）和 Remembered Set （记录哪些变量引用了本Region内的对象）

    + 过程：
      1. 引用对象复制的时候会更新Card Table的值
      2. 当Card Table内dirty的数量达到一定比例的时候会启动一些线程将标为dirty的地址写到被引用Region的RS中
      3. 当dirty数量持续达到一个阈值时启动全部线程更新dirty地址
      4. 再达到一个更大的阈值时会降低用户线程数来更新dirty地址
    + 优点：GC时不用扫描全部Region，只需要根据Rememered Set里的dirty地址进行扫描就行



## G1的GC模式

### Young GC

Young GC 回收的是所有年轻代的Region。**当E区不能再分配新的对象时就会触发**。E区的对象会移动到S区，当S区空间不够的时候，E区的对象会直接晋升到O区，同时S区的数据移动到新的S区，如果S区的部分对象到达一定年龄，会晋升到O区。
 Yung GC过程示意图如下：![1586070710711](最后的波纹.assets/1586070710711.png)



### Mixed GC

Mixed GC 翻译过来叫混合回收。之所以叫混合是因为回收所有的年轻代的Region+部分老年代的Region。
 1、为什么是老年代的**部分**Region？
 2、什么时候触发Mixed GC?
 这两个问题其实可以一并回答。回收**部分**老年代是参数`-XX:MaxGCPauseMillis`，用来指定一个G1收集过程目标停顿时间，默认值200ms，当然这只是一个期望值。G1的强大之处在于他有一个停顿预测模型（Pause Prediction Model），他会有选择的挑选**部分**Region，去尽量满足停顿时间，关于G1的这个模型是如何建立的，这里不做深究。
 Mixed GC的触发也是由一些参数控制。比如`XX:InitiatingHeapOccupancyPercent`表示老年代占整个堆大小的百分比，默认值是45%，达到该阈值就会触发一次Mixed GC。

Mixed GC主要可以分为两个阶段：
 1、全局并发标记（global concurrent marking）
 全局并发标记又可以进一步细分成下面几个步骤：

- 初始标记（initial mark，STW）。它标记了从GC Root开始直接可达的对象。初始标记阶段借用young GC的暂停，因而没有额外的、单独的暂停阶段。
- 并发标记（Concurrent Marking）。这个阶段从GC Root开始对heap中的对象标记，标记线程与应用程序线程并行执行，并且收集各个Region的存活对象信息。过程中还会扫描上文中提到的SATB write barrier所记录下的引用。
- 最终标记（Remark，STW）。标记那些在并发标记阶段发生变化的对象，将被回收。
- 清除垃圾（Cleanup，部分STW）。这个阶段如果发现完全没有活对象的region就会将其整体回收到可分配region列表中。 清除空Region。

2、拷贝存活对象（Evacuation）
 Evacuation阶段是全暂停的。它负责把一部分region里的活对象拷贝到空region里去（并行拷贝），然后回收原本的region的空间。Evacuation阶段可以自由选择任意多个region来独立收集构成收集集合（collection set，简称CSet），CSet集合中Region的选定依赖于上文中提到的**停顿预测模型**，该阶段并不evacuate所有有活对象的region，只选择收益高的少量region来evacuate，这种暂停的开销就可以（在一定范围内）可控。

Mixed GC的清理过程示意图如下：![1586070743108](最后的波纹.assets/1586070743108.png)



### Full GC

G1的垃圾回收过程是和应用程序并发执行的，当Mixed GC的速度赶不上应用程序申请内存的速度的时候，Mixed G1就会降级到Full GC，使用的是Serial GC。Full GC会导致长时间的STW，应该要尽量避免。
 导致G1 Full GC的原因可能有两个：

1. Evacuation的时候没有足够的to-space来存放晋升的对象；
2. 并发处理过程完成之前空间耗尽



# JAVA线程/线程池

创建线程：

1. 继承Thread类创建线程： 定义Thread类的子类，并重写该类的run()方法，该run()方法的方法体就代表了线程需要完成的任务。 

2. 实现Runnable接口创建线程类： 定义Runnable接口的实现类，并重写该接口的run()方法，该run()方法的方法体就是线程的线程执行体。 

3. 实现Callable接口，通过FutureTask/Future来创建有返回值的Thread线程，再通过Executor执行

   补充：与实现Runnable接口类似，都是实现接口，不同的是该方式有返回值，可以获得异步执行的结果。

线程池：ThreadPoolExecutor

 ThreadPoolExecutor中有一个ctl的变量，ctl是AtomicInteger的，所以是线程安全的。ctl维护两个概念上的参数：workCount和runState。workCount表示有效的线程数量，runState表示线程池的运行状态。运行状态只要有五个，分别是RUNNING、SHUTDOWN、STOP、TIDYING和TERMINATED。

RUNNING
接受新任务并且处理已经进入队列的任务
SHUTDOWN
不接受新任务，但是处理已经进入队列的任务
STOP
不接受新任务，不处理已经进入队列的任务，并且中断正在执行的任务
TIDYING
 过渡状态 所有任务执行完成，workerCount为0。线程转到了状态TIDYING会执行terminated()钩子方法
TERMINATED
 终止状态 terminated()方法执行完成
状态之间可以相互转换

ThreadPoolExecutor构造方法中有几个参数，分别是corePoolSize、maximunPoolSize、keepAliveTime、unit、workQueue、threadFactory和handler。下面分别介绍这个几个参数：
**corePoolSize**

核心线程的数量。默认是没有超时的，也就是说就算线程闲置，也不会被处理。但是如果设置了allowCoreTimeOut为true，那么当核心线程闲置时，会被回收。
**maximumPoolSize**：最大线程池大小，被CAPACITY限制（2^29-1）。
**keepAliveTime**
闲置线程被回收的时间限制
**unit**
keepAliveTime的单位
**workQueue**
用于存放任务的队列 **即被添加到线程池中，但尚未被执行的任务；它一般分为直接提交队列、有界任务队列、无界任务队列、优先任务队列几种；** 

 1、**直接提交队列**：设置为SynchronousQueue队列，SynchronousQueue是一个特殊的BlockingQueue，它没有容量，没执行一个插入操作就会阻塞，需要再执行一个删除操作才会被唤醒，反之每一个删除操作也都要等待对应的插入操作。 

 2、**有界的任务队列**：使用ArrayBlockingQueue实现 

 3、**无界的任务队列**：使用LinkedBlockingQueue实现， 线程池的任务队列可以无限制的添加新的任务，而线程池创建的最大线程数量就是你corePoolSize设置的数量，也就是说在这种情况下maximumPoolSize这个参数是无效的 

 4**、优先任务队列：**优先任务队列通过PriorityBlockingQueue实现， 按优先级进行了重新排列执行，也是一个无界的队列。

**threadFactory**
创建线程的工厂类
**handler**
当任务执行失败时，使用handler通知调用者

1、AbortPolicy策略：该策略会直接抛出异常，阻止系统正常工作；

2、CallerRunsPolicy策略：如果线程池的线程数量达到上限，该策略会把任务队列中的任务放在调用者线程当中运行；

3、DiscardOledestPolicy策略：该策略会丢弃任务队列中最老的一个任务，也就是当前任务队列中最先被添加进去的，马上要被执行的那个任务，并尝试再次提交；

4、DiscardPolicy策略：该策略会默默丢弃无法处理的任务，不予任何处理。

内部类Worker

 Worker类封装了线程池工作线程，即一个Worker实例代表一个工作线程。同时Worker类继承于AbstractQueuedSynchronizer，实现了独占锁，一个Worker实例也是一个锁。同时Worker类实现了Runnable接口，重写了run()方法，于是一个Worker实例可以用于线程中执行。 

线程执行：

execute方法内部分3个步骤进行处理。

1. 如果当前正在执行的Worker数量比corePoolSize(基本大小)要小。直接创建一个新的Worker执行任务，会调用addWorker方法
2. 如果当前正在执行的Worker数量大于等于corePoolSize(基本大小)。将任务放到阻塞队列里，如果阻塞队列没满并且状态是RUNNING的话，直接丢到阻塞队列，否则执行第3步。丢到阻塞队列之后，还需要再做一次验证(丢到阻塞队列之后可能另外一个线程关闭了线程池或者刚刚加入到队列的线程死了)。如果这个时候线程池不在RUNNING状态，把刚刚丢入队列的任务remove掉，调用reject方法，否则查看Worker数量，如果Worker数量为0，起一个新的Worker去阻塞队列里拿任务执行
3. 丢到阻塞队列失败的话，会调用addWorker方法尝试起一个新的Worker去阻塞队列拿任务并执行任务，如果这个新的Worker创建失败，调用reject方法

新建工作线程

当需要创建新的工作线程的时候，调用addWorker函数，其主要功能为检查线程池的状态，判断是否满足创建新的工作线程的条件，如果满足则创建一个Worker实例作为新的工作线程来执行任务。



# Java锁机制

1、锁升级

锁的4中状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态（级别从低到高）

 （1）偏向锁：  大多数时候是不存在锁竞争的，常常是一个线程多次获得同一个锁，因此如果每次都要竞争锁会增大很多没有必要付出的代价，为了降低获取锁的代价，才引入的偏向锁。 

当线程1访问代码块并获取锁对象时，会在java对象头和栈帧中记录偏向的锁的threadID，因为偏向锁不会主动释放锁，因此以后线程1再次获取锁的时候，需要比较当前线程的threadID和Java对象头中的threadID是否一致，如果一致（还是线程1获取锁对象），则无需使用CAS来加锁、解锁；如果不一致, 那么需要**查看Java对象头中记录的线程1是否存活**，如果没有存活，那么锁对象被重置为无锁状态，其它线程（线程2）可以竞争将其设置为偏向锁；如果存活，那么立刻**查找该线程（线程1）的栈帧信息，如果还是需要继续持有这个锁对象**，那么暂停当前线程1，撤销偏向锁，升级为轻量级锁 .

（2）轻量级锁：轻量级锁考虑的是竞争锁对象的线程不多，而且线程持有锁的时间也不长的情景。因为阻塞线程需要CPU从用户态转到内核态，代价较大，如果刚刚阻塞不久这个锁就被释放了，那这个代价就有点得不偿失了，因此这个时候就干脆不阻塞这个线程，让它自旋这等待锁释放。

线程1获取轻量级锁时会先把锁对象的对象头MarkWord复制一份到线程1的栈帧中创建的用于存储锁记录的空间，然后使用CAS把对象头中的内容替换为线程1存储的锁记录的地址；

如果在线程1复制对象头的同时（在线程1CAS之前），线程2也准备获取锁，复制了对象头到线程2的锁记录空间中，但是在线程2CAS的时候，发现线程1已经把对象头换了，线程2的CAS失败，那么线程2就尝试使用自旋锁来等待线程1释放锁。

自旋的次数是有限制的，如果自旋次数到了线程1还没有释放锁，或者线程1还在执行，线程2还在自旋等待，这时又有一个线程3过来竞争这个锁对象，那么这个时候轻量级锁就会膨胀为重量级锁。重量级锁把除了拥有锁的线程都阻塞，防止CPU空转。

 **偏向所锁，轻量级锁都是乐观锁，重量级锁是悲观锁。** 



可重入锁

 是指同一个线程在外层的方法获取到了锁，在进入内层方法会自动获取到锁。对于ReentrantLock和synchronized关键字都是可重入锁的。**最大的好处就是能够避免一定程度的死锁。**



ReentrantLock特性：

- 可重入
- 可通过构造参数设置时公平锁还是非公平锁
- 需要明文释放锁，而synchronized是自动释放的
- 可响应中断
- 可在获取锁是设置超时时间

 ReentrantLock内部只有一个成员变量Sync，作为一个核心内部类，绝大部分方法都直接依赖这个类实现。

 ReentrantLock内部共3个内部类，其中Sync是一个抽象方法类，继承自AbstractQueuedSynchronizer，类NonFairSync与FairSync均继承自Sync 

类Sync关键方法有3个 分别是lock、nonfairTryAcquire、tryRelease，其中lock是抽象方法。



lock() 操作:

 首先用一个CAS操作，判断state是否是0（表示当前锁未被占用），如果是0则把它置为1，并且设置当前线程为该锁的独占线程，表示获取锁成功。当多个线程同时尝试占用同一个锁时，CAS操作只能保证一个线程操作成功。 

失败的锁执行aquire()方法，这个方法首先会先尝试获取锁，如果获取成功，就直接返回，失败调用addWaiter()进入等待队列。

# synchronized

synchronized是JDK层实现的一把锁，使用了锁升级策略。Synchronized 的作用主要有三个：

- **确保线程互斥的访问同步代码**
- **保证共享变量的修改能够及时可见**
- **有效解决重排序问题**

可以修饰的对象：

1. 普通同步方法（实例方法），锁是当前实例对象 ，进入同步代码前要获得当前实例的锁
2. 静态同步方法，锁是当前类的class对象 ，进入同步代码前要获得当前类对象的锁
3. 同步方法块，锁是括号里面的对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。

 synchronized使用的锁对象是存储在Java对象头里的 ， 并基于Monitor实现了

每个对象有一个监视器锁（Monitor），当 Monitor 被占用时就会处于锁定状态。

线程执行 Monitorenter 指令时尝试获取 Monitor 的所有权，过程如下：

- 如果 Monitor 的进入数为 0，则该线程进入 Monitor，然后将进入数设置为 1，该线程即为 Monitor 的所有者。
- 如果线程已经占有该 Monitor，只是重新进入，则进入 Monitor 的进入数加 1。
- 如果其他线程已经占用了 Monitor，则该线程进入阻塞状态，直到 Monitor 的进入数为 0，再重新尝试获取 Monitor 的所有权。

执行 Monitorexit 的线程必须是 Object ref 所对应的 Monitor 的所有者。

指令执行时，Monitor 的进入数减 1，如果减 1 后进入数为 0，那线程退出 Monitor，不再是这个 Monitor 的所有者。

其他被这个 Monitor 阻塞的线程可以尝试去获取这个 Monitor 的所有权。

# volatile

 JMM：

Java线程之间的通信由Java内存模型(JMM)控制，JMM决定一个线程对共享变量的修改何时对另外一个线程可见。JMM定义了线程与主内存的抽象关系：线程之间的变量存储在主内存(Main Memory)中，每个线程都有一个私有的本地内存(Local Memory)保存着共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。 

如果线程A与线程B通信：

1. 线程A要先把本地内存A中更新过的共享变量刷写到主内存中。
2. 线程B到主内存中读取线程A更新后的共享变量

volatile作用是保证可见性。

可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。 

有volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令，该指令在多核处理器下会引发两件事情。

1. 将当前处理器缓存行数据刷写到系统主内存。
2. 这个刷写回主内存的操作会使其他CPU缓存的该共享变量内存地址的数据无效。

 这样就保证了多个处理器的缓存是一致的，对应的处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器缓存行设置无效状态，当处理器对这个数据进行修改操作的时候会重新从主内存中把数据读取到缓存里。 

# Linux常用命令

## 查看文件

cat,more,less,head,tail

**grep** 指令用于查找内容包含指定的范本样式的文件 

## 查看进程：ps

-a，查看所有

-u，以用户（user）的格式显示

-x, 显示后台进程运行参数

-ef，以全格式显示进程所有信息，包括父进程Pid，创建人，创建时间，进程号。等等

如果想查看包含其他使用者的进程，和PID，CPU占有率，记忆体使用情况，运行状态等 
ps -aux | grep helloworld  

Linux上进程的五种状态：

1.R——Runnable（运行）：正在运行或在运行队列中等待

2.S——sleeping（中断）：休眠中，受阻，在等待某个条件的形成或接收到信号

3.D——uninterruptible sleep(不可中断)：收到信号不唤醒和不可运行，进程必须等待直到有中断发生

4.Z——zombie（僵尸）：进程已终止，但进程描述还在，直到父进程调用wait4()系统调用后释放

5.T——traced or stoppd(停止)：进程收到SiGSTOP,SIGSTP,SIGTOU信号后停止运行

状态后缀表示：

<：优先级高的进程

N：优先级低的进程

L：有些页被锁进内存


当前所有的进程. 包括显示创建进程的用户标识uid, 进程标识pid, 父进程标识ppid, 创建时间,所执行程序 

ps -ef |grep helloworld



## 查看端口占用

lsof -i:端口号 用于查看某一端口的占用情况，比如查看8000端口使用情况，lsof -i:8000 

netstat -tunlp |grep 端口号，用于查看指定的端口号的进程情况，如查看8000端口的情况，netstat -tunlp |grep 8000



## top：显示系统中各个进程的资源占用状况

top命令是Linux下常用的性能分析工具，能够实时显示系统中**各个进程的资源占用状况**，类似于Windows的任务管理器。  top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态。

**功能**：显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 

**常用命令参数：**

 **-d：number代表秒数，表示top命令显示的页面更新一次的间隔。默认是5秒。 -b：以批次的方式执行top。 -n：与-b配合使用，表示需要进行几次top命令的输出结果。 -p：指定特定的pid进程号进行观察。** 

-i<时间> 设置间隔时间

-u<用户名> 指定用户名

前5行统计信息
第1行：top - 05:43:27 up 4:52, 2 users, load average: 0.58, 0.41, 0.30 
第1行是任务队列信息，其参数如下：

当前时间  系统运行时间   当前登录用户数   系统负载
load average: 如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 

第2行：Tasks: 159 total, 1 running, 158 sleeping, 0 stopped, 0 zombie 
第3行：%Cpu(s): 37.0 us, 3.7 sy, 0.0 ni, 59.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 

第2、3行为进程和CPU的信息 

包括进程总数，正在运行的进程数，睡眠的进程数，停止的进程数，用户空间内核空间占用CPU百分比，硬中断、软中断占用cpu百分比

第4、5行为内存信息  

包括物理内存总量，使用的内存总量，空闲内存总量，用作内核缓存的内存量，交换区总量，使用、空闲、缓冲的交换区总量，可用于进程下一次分配的物理内存数量

下面的行都是进程信息

包括进程号，进程用户，父进程id,优先级，进程状态，进程使用的内存百分比，进程使用的cpu时间

## netstat：显示与IP、TCP、UDP和ICMP协议相关的统计数据

 netstat是控制台命令,是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。Netstat用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。 

常用参数

 -a (all) 显示所有选项，默认不显示LISTEN相关。
-t (tcp) 仅显示tcp相关选项。
-u (udp) 仅显示udp相关选项。
 -n 以网络IP地址代替名称，显示出网络连接情形。 
-l 仅列出有在 Listen (监听) 的服务状态

-p 与其他参数一起使用， 就可以添加“pid、进程名称”到netstat输出中
-r 显示路由信息
netstat -ap | grep ssh 找出程序运行的端口
netstat -an | grep  80 找出运行在指定端口的进程

netstat的输出结果可以分为两个部分

1、Active Internet connections 有源TCP连接，其中"Recv-Q"和"Send-Q"指接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。

2、Active UNIX domain sockets 有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。

列名解释：

Proto：显示连接使用的协议。

RefCnt：表示连接到本套接口上的进程号。

Types：显示套接口的类型。

State：显示套接口当前的状态。

Path：表示连接到套接口的其它进程使用的路径名。



state参考TCP三次握手状态

## iostat(用的少)显示磁盘活动统计情况

iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是显示磁盘活动统计情况，同时也会显示出CPU使用情况。iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析.

CPU属性说明：

| %user   | CPU在用户态执行进程的时间百分比。                            |
| ------- | ------------------------------------------------------------ |
| %nice   | CPU在用户态模式下，用于nice操作，所占用CPU总时间的百分比     |
| %system | CPU处在内核态执行进程的时间百分比                            |
| %iowait | CPU用于等待I/O操作占用CPU总时间的百分比                      |
| %steal  | 管理程序(hypervisor)为另一个虚拟进程提供服务而等待虚拟CPU的百分比 |
| %idle   | CPU空闲时间百分比                                            |

1. 若 %iowait 的值过高，表示硬盘存在I/O瓶颈 
2. 若 %idle 的值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量 
3. 若 %idle 的值持续低于1，则系统的CPU处理能力相对较低，表明系统中最需要解决的资源是 CPU 

## vmstat：对操作系统的虚拟内存、进程、CPU活动进行监控

vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可对操作系统的虚拟内存、进程、CPU活动进行监控。是对系统的整体情况进行统计。  vmstat 是用来实时查看内存使用情况,反映的情况比用top直观一些.
如果直接使用,只能得到当前的情况,最好用个时间间隔来采集 

**常用参数**

-a：显示活跃和非活跃内存

-f：显示从系统启动至今的fork数量 。

-n：只在开始时显示一次各字段名称。

-s：显示内存相关统计信息及多种系统活动数量。

delay：刷新时间间隔。如果不指定，只显示一条结果。

count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。

**字段说明：**

**Procs（进程）：**

r: 运行队列中进程数量

b: 等待IO的进程数量

**Memory（内存）：**

swpd: 使用虚拟内存大小

free: 可用内存大小

buff: 用作缓冲的内存大小

cache: 用作缓存的内存大小

**Swap：**

si: 每秒从交换区写到内存的大小

so: 每秒写入交换区的内存大小

IO：（现在的Linux版本块的大小为1024bytes）

bi: 每秒读取的块数

bo: 每秒写入的块数

**系统：**

in: 每秒中断数，包括时钟中断。

cs: 每秒上下文切换数。

CPU（以百分比表示）：

us: 用户进程执行时间(user time)

sy: 系统进程执行时间(system time)

id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。

wa: 等待IO时间


## free：显示系统内存的使用情况

 free 命令显示系统内存的使用情况，包括物理内存、交换内存(swap)和内核缓冲区内存。 

**命令参数：**

-b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。-m 以MB为单位显示内存使用情况。-g  以GB为单位显示内存使用情况。 

-o 不显示缓冲区调节列。 

-s<间隔秒数> 持续观察内存使用状况。 

-t 显示内存总和列。 

**输出信息**

**Mem** 行(第二行)是内存的使用情况。
**Swap** 行(第三行)是交换空间的使用情况。
**total** 列显示系统总的可用物理内存和交换空间大小。
**used** 列显示已经被使用的物理内存和交换空间。
**free** 列显示还有多少物理内存和交换空间可用使用。
**shared** 列显示被共享使用的物理内存大小。
**buff/cache** 列显示被 buffer 和 cache 使用的物理内存大小。
**available** 列显示还可以被应用程序使用的物理内存大小。 



## tcpdump

 [tcpdump](http://en.wikipedia.org/wiki/Tcpdump) 是一个运行在命令行下的抓包工具。它允许用户拦截和显示发送或收到过网络连接到该计算机的TCP/IP和其他数据包 

 **1）从特定接口捕获数据包** 

 \# tcpdump -i {接口名} 

 **2）从特定接口捕获特定数量数据包** 

 \# tcpdump -c 12 -i enp0s3 

 **3）显示 tcpdump 的所有可用接口** 

 使用 -D 选项 

 **4）捕获带有可读时间戳的数据包（-tttt 选项）** 

 **5）捕获数据包并将其保存到文件（-w 选项）** 

 **6）从保存的文件中读取数据包（-r 选项）** 

 **7）仅捕获特定接口上的 IP 地址数据包（-n 选项）** 

 **8）仅捕获特定接口上的 TCP 数据包** （tcp)



**1、抓取回环网口的包：tcpdump -i lo**

**2、防止包截断：tcpdump -s0**

**3、以数字显示主机及端口：tcpdump -n**



# Linux进程线程

## 虚拟内存

 虚拟内存（virtual memory，VM），是一种内存管理技术。它是操作系统提供的一种对主存的抽象。虚拟内存的实现由操作系统软件和硬件结合完成，包括硬件异常、地址翻译、磁盘文件、内核程序等。 

 虚拟内存解决了什么问题？ **缓存**，**内存管理**，**内存保护** 

1）虚拟内存给进程提供了一个更大的内存空间，不再受物理内存大小的限制。它将物理内存看作是存储在磁盘上的地址空间的缓存。 现在的电脑好一点的差不多就是 16GB 或者 32GB 的内存，而且内存越大，肯定就越贵。那如果只有物理内存，在很多情况下根本不够用，特别是需要运行很多程序的情况下。而磁盘空间相对来说是很便宜的，即使是 SSD，在同样的容积下也便宜太多了。虚拟内存技术在主存中只保留活动区域，然后根据需要在磁盘和主存之间来回传送数据，这样，它就可以更加高效的利用主存。

2）虚拟内存为程序提供内存管理。我们在敲代码的时候，不需要考虑这个变量会不会被其它程序错误的修改。因为虚拟内存帮我们做了这些事情，它给程序提供了内存隔离，为程序提供了安全的共享物理内存的途径。使得每个进程的地址空间不会被其它进程破坏。 比如说我们在程序中定义了一个指针，并且为它分配了空间，这块内存最终会分配到物理内存上。你不用担心其它程序会分配相同的物理内存。

3）虚拟内存技术也给每个进程提供了一致的、完整的地址空间。比如在操作系统上执行若干个进程，每个进程都有相同的地址空间，都在同样的起始位置放置了堆、栈以及代码段等。这样，它简化了像链接器、加载器这样的程序的内存管理。

## 进程线程区别：

进程是资源分配的基本单位。  线程是独立调度的基本单位。  一个进程中可以有多个线程，它们共享进程资源。 Linux进程所需具备的四要素：
   （1）程序代码。代码不一定是进程专有，可以与其它进程共用。
   （2）系统堆栈空间，这是进程专用的。
   （3）在内核中维护相应的进程控制块。只有这样，该进程才能成为内核调度的基本单位，接受调度。并且，该结构也记录了进程所占用的各项资源。
   （4）有独立的存储空间，表明进程拥有专有的用户空间。   

缺少一个就是线程

区别：

Ⅰ 拥有资源

进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。

Ⅱ 调度

线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。

Ⅲ 系统开销

由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。

Ⅳ 通信方面

线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。

线程优点：

- 线程间共享数据很简单，而进程需要借助IPC
- 创建线程，线程间上下文切换的时间都要快于进程

线程缺点：

- 多线程编程时，需要确保调用线程安全的函数，或者以线程安全的方式调用函数，多进程无需关注。
- 某个线程的bug可能危及该进程的所有线程，因为它们共享着相同的地址空间和其他属性。
- 每个线程都在争用宿主进程中有限的虚拟地址空间，特别是每个线程栈消耗掉进程虚拟地址空间一部分，后续线程将不能使用这些。但每个进程可以使用全部的有效虚拟内存。
- 在多线程应用中，所有线程必须运行同一个程序，而对于多进程，不同进程可以运行不同程序。

## 进程状态切换

运行态—→等待态：等待使用资源；如**等待外设传输**；等待人工干预。
等待态—→就绪态：资源得到满足；如**外设传输结束**；人工干预完成。
运行态—→就绪态：**运行时间片到**；**出现有更高优先权进程**。
就绪态—→运行态：**CPU 空闲时选择一个就绪进程**。 

## 创建进程

​         linux主要是使用fork()系统函数来创建进程。

​         fork()本质上是复制进程，它会复制当前进程的副本，以适当的方式将这些资源交给子进程。所以子进程掌握的资源和父进程是一样的，包括内存中的内容，**所以也包括环境变量和变量**。但父子进程是完全独立的，它们是一个程序的两个实例。

​          完成fork调用后，存在两个进程，每个进程都会从fork的返回处继续执行。程序代码通过fork()的返回值来区分父子进程，父进程中，fork()返回的是子进程的进程ID,子进程中返回0。子进程可以调用getpid()获得自身进程ID，调用getppid()获取父进程ID。创建失败时，fork()返回-1.（进程数量超出限制）

​          传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。Linux的fork()使用写时复制（copy-on-write）页实现。 

​           写时复制技术：内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟空间结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。 

​           在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间，如果没有exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。    

​           exec是加载另一个应用程序，替代当前运行的进程，也就是说在不创建新进程的情况下加载一个新程序。exec还有一个动作，在进程执行完毕后，退出exec所在环境(实际上是进程直接跳转到exec上，执行完exec就直接退出。而非exec加载程序的方式是：父进程睡眠，然后执行子进程，执行完后回到父进程，所以不会立即退出当前环境)。所以为了保证进程安全，若要形成新的且独立的子进程，都会先fork一份当前进程，然后在fork出来的子进程上调用exec来加载新程序替代该子进程。



​            linux还可以用vfork()来创建进程， 用vfork创建的子进程与父进程共享地址空间，也就是说子进程完全运行在父进程的地址空间上，如果这时子进程修改了某个变量，这将影响到父进程。 

​           有一点要注意的是用vfork()创建的子进程必须显示调用exit()来结束，否则子进程将不能结束，而fork()则不存在这个情况。vfork也是在父进程中返回子进程的进程号，在子进程中返回0。用 vfork创建子进程后，父进程会被阻塞直到子进程调用exec或exit。有了写时复制，vfork()很少用。

​           clone也可以用来创建进程，功能要更强大。系统调用fork()和vfork()是无参数的，而clone()则带有参数，选择哪些复制哪些内容，所以clone()的主要用途是创建一个线程，这个线程可以是内核线程，也可以是用户线程。 fork()是全部复制，vfork()是共享内存，而clone()是则可以将父进程资源有选择地复制给子进程，而没有复制的数据结构则通过指针的复制让子进程共享，具体要复制哪些资源给子进程，由参数列表中的clone_flags来决定。另外，clone()返回的是子进程的pid。 



这三个API的内部实际都是调用一个内核内部函数**do_fork**，只是填写的参数不同而已。

```c
long do_fork(unsigned long clone_flags,
          unsigned long stack_start,                           
          unsigned long stack_size,
          int __user *parent_tidptr,
          int __user *child_tidptr)
```

clone_flag 是clone进程的标志参数（这是 一个很重要的参数，如果我遇到了一些函数在判断clone_flags的if里边调用的，理解不了的话可以去看看这个flag的定义，定义会解释这个flag是为了标记什么，良心出品，帮助理解的利器），clone_flags定义了很多，在这里我就不赘述了。
stack_start 是栈的起始地址
stack_size 栈的大小（初值被设定为0）
parent_tidptr 初值被设置为NULL
child_tidptr 初值被设置为NULL

do_fork主要的作用就是复制原来的进程成为另一个新的进程，它完成了整个进程的创建过程。do_fork()函数利用辅助函数copy_process()来创建进程描述符以及子进程执行所需要的所有其他内核数据结构。下面是do_fork()执行的主要步骤：

  

 1、在do_fork()函数中首先创建一个task_struct结构体指针，再对传递给do_fork的flag参数进行处理和检查，看当前进程是否设置了跟踪标记ptrace。 

 task_struct结构体是用来描述进程的结构体，进程需要记录的信息都在其中，下面我们来看看其中的具体项目。结构体存储在linux/sched.h中。

```c
volatile long state; 
void *stack;
...
struct task_struct __rcu *real_parent;
struct task_struct __rcu *parent;
struct list_head children;
struct list_head sibling;
struct task_struct *group_leader;
```

 意思分别是
进程的状态：可运行，不可运行      栈头
真正的父进程   养父进程     子进程的链表    兄弟进程链表     线程组长 

 2、 接下来检查当前进程（父进程）的ptrace字段。ptrace是用来标示一个进程是否被另外一个进程所跟踪。所谓跟踪，最常见的例子就是处于调试状态下的进程被debugger进程所跟踪。父进程的ptrace字段非0时说明debugger程序正在跟踪父进程，那么接下来通过fork_traceflag函数来检测子进程是否也要被跟踪。如果trace为1，那么就将跟踪标志CLONE_PTRACE加入标志变量clone_flags中。 

3、 通过copy_process()创建子进程的描述符，并创建子进程执行时所需的其他数据结构，最终则会返回这个创建好的进程描述符。 实现将父进程的寄存器以及所有进程执行环境的相关部分复制给子进程。

在copy_process()函数中：首先对一些标志进行合法性检查，检查完之后调用dup_task_struct函数来为新进程创建一个内核栈、thread_info和task_struct，这里完全copy父进程的内容。调用完这个函数后子进程和父进程是完全相同的，之后 初始化子进程描述符。初始化其中的各个字段，使得子进程和父进程逐渐区别出来。 

然后 复制进程的所有信息。根据clone_flags的具体取值来为子进程拷贝或共享父进程的某些数据结构。

写时复制就发生在这里，调用copy_mm函数对页表进行拷贝，我本来以为fork()是默认传入了CLONE_VM参数，使子进程与父进程运行在相同的内存空间来实现的，但实际不是，如果使用了CLONE_VM，父进程就不会继续调用dup_mm为子进程分配一个新的地址空间，但实际上写时拷贝有自己的虚拟地址空间，只是共享物理空间，所以父进程会继续调用dup_mm， 先给子进程分配了一个新的结构体，然后调用dup_mmap拷贝父进程地址空间 ，在这个函数内有个 copy_page_range （）函数负责页拷贝，因为linux有四级页表，页目录PGD， 页上级目录pud、页中间目录pmd、 页表项 pte，所以这个函数会分别进行拷贝，直到 copy_pte_range中，有一段代码，

```c
if (is_cow_mapping(vm_flags)) {
	ptep_set_wrprotect(src_mm, addr, src_pte);
	pte = pte_wrprotect(pte);
}
```

 代码判断如果父进程的页支持写时复制，就将父子进程的页都置为写保护状态，这样子进程的物理页面在创建期间就不会动了。

之后还会复制父进程的线程， 通过copy_threads()函数更新子进程的内核栈和寄存器中的值。 最后 用alloc_pid函数为这个新进程分配一个pid，Linux系统内的pid是循环使用的，采用位图方式来管理。简单的说，就是用每一位（bit）来标示该位所对应的pid是否被使用。 

 用p来接收返回值，返回值类型是task_struct类型，并且p用来存储新进程的pid。 

![](https://img-blog.csdn.net/20141006153809442?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWFuZ2NzMjAwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

4、如果copy_process函数执行成功，那么将继续下面的代码。
首先定义了一个完成量vfork，如果clone_flags包含CLONE_VFORK标志，那么将进程描述符中的vfork_done字段指向这个完成量，之后再对vfork完成量进行初始化。

完成量的作用是，直到任务A发出信号通知任务B发生了某个特定事件时，任务B才会开始执行；否则任务B一直等待。

5、  如果子进程被跟踪或者设置了CLONE_STOPPED标志，那么通过sigaddset函数为子进程增加挂起信号。signal对应一个unsigned long类型的变量，该变量的每个位分别对应一种信号。具体的操作是，将SIGSTOP信号所对应的那一位置1。 

 6． 如果子进程并未设置CLONE_STOPPED标志，那么通过wake_up_new_task函数使得父子进程之一优先运行；否则，将子进程的状态设置为TASK_STOPPED。 

 7． 如果父进程被跟踪，则将子进程的pid赋值给父进程的进程描述符的pstrace_message字段。再通过ptrace_notify函数使得当前进程定制，并向父进程的父进程发送SIGCHLD信号。 

8、如果设置了CLONE_VFORK标志，则把父进程插入等待队列，并挂起父进程直到子进程释放自己的内存地址空间（也就是说，直到子进程结束或执行新的程序）。

9、结束并返回子进程的PID。

## 子进程监控

**等待子进程**

wait()函数用于使父进程（也就是调用wait()的进程）阻塞，直到一个子进程结束或者该进程接收到了一个指定的信号为止。如果该父进程没有子进程或者它的子进程已经结束，则wait()函数就会立即返回。

 调用wait后，进程立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait 就会收集这个子进程的信息， 并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。 

 waitpid()的作用和wait()一样，但它并不一定要等待第一个终止的子进程（它可以指定需要等待终止的子进程），它还有若干选项，如可提供一个非阻塞版本的 wait()功能，也能支持作业控制。实际上，wait()函数只是 waitpid()函数的一个特例，在Linux 内部实现 wait()函数时直接调用的就是waitpid()函数。



**孤儿进程**

孤儿进程：当父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作，因此孤儿进程并没有什么危害，系统中会经常有这种情况。

**僵尸进程**

僵尸进程：当一个子进程结束运行（一般是调用exit、运行时发生致命错误或收到终止信号所导致）时，子进程的退出状态（返回值）会报给操作系统，系统则以SIGCHLD信号告知父进程，此时子进程的进程控制块（PCB）仍驻留在内存中。父进程收到SIGCHLD后，会调用wait()函数获取子进程的退出状态，然后内核就可以从内存中释放已结束的子进程的PCB；而如若父进程没有这么做的话，子进程的PCB就会一直驻留在内存中，即成为僵尸进程。

**解决僵尸进程**

 尽量在父进程中使用waitpid()避免产生僵尸进程。

可以使用top命令来查看服务器当前是否有僵尸进程，下图中可以看到僵尸进程的提示，如果数字大于0，那么意味着服务器当前存在僵尸进程 ，用ps 命令和 grep命令寻找僵尸进程：

```
ps -A | grep -e '^[Zz]'
```

可以得到这个僵尸进程的进程号以及父进程号

 可以使用`kill -HUP 僵尸进程号` 来杀掉这个僵尸进程； 如果不行，杀掉父进程， 父进程死后，僵尸进程成为”孤儿进程”，过继给1号进程init，init始终会负责清理僵尸进程．它产生的所有僵尸进程也跟着消失。 

 kill -HUP :如果想要更改配置而不需停止并重新启动服务，则使用该命令。 

## 进程通信

Linux进程间基本的通信方式主要有：管道(pipe)(包括匿名管道和命名管道)、信号(signal)、消息队列(queue)、共享内存、信号量和套接字。

1.**管道**：管道的实质是一个内核缓冲区，管道的作用正如其名，需要通信的两个进程在管道的两端，进程利用管道传递信息。管道对于管道两端的进程而言，就是一个文件，但是这个文件比较特殊，它不属于文件系统并且只存在于内存中。

管道分为匿名管道pipe和命名管道fifo

匿名管道，是一种最基本的IPC机制，由pipe函数创建：

#include <unistd.h>
int pipe(int pipefd[2]);

返回值：成功返回0，失败返回-1；
调用pipe函数时在内核中开辟一块缓冲区用于通信,它有一个读端，一个写端：pipefd[0]指向管道的读端，pipefd[1]指向管道的写端。所以管道在用户程序看起来就像一个打开的文件,通过read(pipefd[0])或者write(pipefd[1])向这个文件读写数据，其实是在读写内核缓冲区。

 **使用匿名管道的通信过程：** 

1.父进程调用pipe开辟管道,得到两个文件描述符指向管道的两端。
2.父进程调用fork创建子进程,那么子进程也有两个文件描述符指向同一管道。
3.父进程关闭管道读端,子进程关闭管道写端。父进程可以往管道里写,子进程可以从管道里读,管道是用环形队列实现的,数据从写端流入从读端流出,这样就实现了进程间通信。



**命名管道**允许没有亲缘关系的进程进行通信。命名管道不同于匿名管道之处在于它提供了一个路径名与之关联，这样一个进程即使与创建有名管道的进程不存在亲缘关系，只要可以访问该路径，就能通过有名管道互相通信。

       int mknod(const char *pathname, mode_t mode, dev_t dev);
    
       int mkfifo(const char *pathname, mode_t mode);
返回值：都是成功返回0，失败返回-1；
path为创建的命名管道的全路径名；
mod为创建的命名管道的模式，指明其存取权限；
dev为设备值，该值取决于文件创建的种类，它只在创建设备文件时才会用到； 

命名管道是一个存在于硬盘上的文件，而管道是存在于内存中的特殊文件。所以当使用命名管道的时候必须先open将其打开。 

**2.信号**：

1.概念：

　　1）信号是在软件层次上对中断机制的一种模拟，是一种异步通信方式

　　2）信号可以直接进行用户空间进程和内核进程之间的交互，内核进程也可以利用它来通知用户空间进程发生了哪些系统事件。比如wait()

　　3）如果该进程当前并未处于执行态，则该信号就由内核保存起来，直到该进程恢复执行再传递给它；如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被 取消时才被传递给进程。

2.用户进程对信号的响应方式：

　　1）忽略信号：对信号不做任何处理，但是有两个信号不能忽略：即SIGKILL及SIGSTOP。

　　2）捕捉信号：定义信号处理函数，当信号发生时，执行相应的处理函数。

　　3）执行缺省操作：Linux对每种信号都规定了默认操作

常见信号

​        SIGINT：ctrl+c 终止信号   信号值2

　　SIGQUIT：ctrl+\ 终止信号     信号值3

　　SIGTSTP:ctrl+z 暂停信号        信号值20

　　SIGALRM：闹钟信号 收到此信号后定时结束，结束进程   信号值14

　　SIGCHLD：子进程状态改变，父进程收到信号    信号值17

　　SIGKILL：杀死信号   信号值9

信号发送：kill(pid_t pid, int sig)

**3.消息队列：**是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。

特点

1. 消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。
2. 消息队列独立于发送与接收进程。进程终止时，消息队列及其内容并不会被删除。
3. 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取。

**4.共享内存：**使得多个进程可以可以直接读写同一块内存空间，是针对其他通信机制运行效率较低而设计的。

为了在多个进程间交换信息，内核专门留出了一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大提高效率。

特点

1. 共享内存是最快的一种 IPC，因为进程是直接对内存进行存取。
2. 因为多个进程可以同时操作，所以需要进行同步。
3. 因为共享内存并未提供同步机制，所以信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

**5.信号量：**

信号量的本质是一种数据操作锁，用来负责数据操作过程中的互斥，同步等功能。
信号量用来管理临界资源的。它本身只是一种外部资源的标识，不具有数据交换功能，而是通过控制其他的通信资源实现进程间通信。 可以这样理解，信号量就相当于是一个计数器。当有进程对它所管理的资源进行请求时，进程先要读取信号量的值：大于0，资源可以请求；等于0，资源不可以用，这时进程会进入睡眠状态直至资源可用。
当一个进程不再使用资源时，信号量+1(对应的操作称为V操作)，反之当有进程使用资源时，信号量-1(对应的操作为P操作)。对信号量的值操作均为原子操作。

**6.套接字：**套接字是更为基础的进程间通信机制，与其他方式不同的是，套接字可用于不同机器之间的进程间通信。

有两种类型的套接字：基于文件的和面向网络的。

(1).Unix套接字是基于文件的，并且拥有一个“家族名字”--AF_UNIX，它代表地址家族(address family)：UNIX。

(2).第二类型的套接字是基于网络的，它也有自己的家族名字--AF_INET，代表地址家族(address family):INTERNET

不管采用哪种地址家族，都有两种不同的套接字连接：面向连接的和无连接的。

(1)面向连接的套接字(SOCK_STREAM)：进行通信前必须建立一个连接，面向连接的通信提供序列化的、可靠地和不重复的数据交付，而没有记录边界。

这意味着每条信息可以被拆分成多个片段，并且每个片段都能确保到达目的地，然后在目的地将信息拼接起来。

实现这种连接类型的主要协议是传输控制协议(TCP)。

(2)无连接的套接字(SOCK_DGRAM)：在通信开始之前并不需要建立连接，在数据传输过程中并无法保证它的顺序性、可靠性或重复性。

##  线程操作

创建：pthread_create

终止：pthread_exit

线程号：pthread_self

连接：pthread_join

 pthread_join使一个线程等待另一个线程结束。 相当于进程的wait()，但是线程间关系是对等的

分离：pthread_detach

 pthread_detach(threadid)函数的功能是使线程ID为threadid的线程处于分离状态，一旦线程处于分离状态，该线程终止时底 层资源立即被回收； 

## 线程同步

1. 互斥锁

   - Linux中提供一把互斥锁mutex（也称之为互斥量）。每个线程在对资源操作前都尝试先加锁，成功加锁才能操作，操作结束解锁。资源还是共享的，线程间也还是竞争的， 但通过“锁”就将资源的访问变成互斥操作，而后与时间有关的错误也不会再产生了。
   - 但，应注意：同一时刻，只能有一个线程持有该锁。
   - 当A线程对某个全局变量加锁访问，B在访问前尝试加锁，拿不到锁，B阻塞。C线程不去加锁，而直接访问该全局变量，依然能够访问，但会出现数据混乱。
   - 互斥锁实质上是操作系统提供的一把“建议锁”（又称“协同锁”），建议程序中有多线程访问共享资源的时候使用该机制。

   lock与unlock：

   - lock尝试加锁，如果加锁不成功，线程阻塞，阻塞到持有该互斥量的其他线程解锁为止。
   - unlock主动解锁函数，***\*同时将阻塞在该锁上的所有线程\*******\**\*全部唤醒\*\**\***，至于哪个线程先被唤醒，取决于优先级、调度。默认：先阻塞、先唤醒。

   lock与trylock：

   - lock加锁失败会阻塞，等待锁释放。
   - trylock加锁失败直接返回错误号（如：EBUSY），不阻塞。

2. 条件变量   (比较像volatile)

​         互斥锁防止多个线程同时访问同意共享变量，条件变量允许一个线程就某个共享变量的状态变化通知其他线程，并让其他线程等待这一通知。

​          条件变量是一种“事件通知机制”，它本身不提供、也不能够实现“互斥”的功能。因此，条件变量通常（也必须）配合互斥量来一起使用，其中互斥量实现对“共享数据”的互斥（即同步），而条件变量则去执行 “通知共享数据状态信息的变化”的任务。 

​          条件变量的主要操作是发送信号和等待，发送信号操作即通知一个或多个处于等待状态的线程，某个共享变量的状态已经改变，等待操作值在收到一个通知前一直处于阻塞状态。

## 进程调度：

先来先服务，短作业优先，高优先权优先（抢占非抢占），时间片轮转法，多级反馈队列调度。

## 死锁

线程死锁是指由于两个或者多个线程互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行。

 **死锁的产生是必须要满足一些特定条件的：** 
1.互斥条件：进程对于所分配到的资源具有排它性，即一个资源只能被一个进程占用，直到被该进程释放 
2.请求和保持条件：一个进程因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 
3.不剥夺条件：任何一个资源在没被该进程释放之前，任何其他进程都无法对他剥夺占用 
4.循环等待条件：当发生死锁时，所等待的进程必定会形成一个环路（类似于死循环），造成永久阻塞。

避免死锁的方法：破坏死锁产生条件预防死锁或者避免死锁产生的银行家算法。

预防死锁的方法：

- **固定加锁的顺序**(针对锁顺序死锁)：破坏 循环等待
- **使用定时锁**：破坏不可剥夺



# BIO NIO AIO （linux epoll)

**BIO:**

阻塞IO模型，在读写数据时客户端会发生阻塞。

工作流程：在用户线程发出I/O请求后，内核会检查数据是否就绪，此时用户线程会一直阻塞等待内存数据就绪。在内存数据就绪后，内存将数据复制到用户线程中，并返回I/O执行结果到用户线程，此使用户线程将接触阻塞状态并开始处理数据，最典型的阻塞I/O模型就是data = socket.read(),如果内核数据没有就绪，Socket线程就会一直阻塞在read中等待内核数据就绪。

Java中BIO中操作的流主要有两大类，字节流和字符流，两类根据流的方向都可以分为输入流和输出流

按照类型和输入输出方向可分为字节流和字符流

输入字节流：InputStream

输出字节流：OutputStream

输入字符流：Reader

输出字符流：Writer

字节流主要用来处理字节或二进制对象，字符流用来处理字符文本或字符串

BufferedOutputStream 等带缓冲区的实现，

1. 可以避免频繁的磁盘读写，进而提高 IO 处理效率。
2. 这种设计利用了缓冲区，将批量数据进行一次操作



**NIO:**

普通的非阻塞I/O模型指的是用户线程在发起一个I/O操作后，无需阻塞便可以马上得到内核返回一个结果，如果内核数据还没转备好，内核返回false,稍后再进行I/O操作。现在更常见的是多路I/O复用模型。 I/O多路复用的本质是通过系统内核来缓冲I/O数据，使得单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作 。

 select，poll，epoll都是IO多路复用的机制。**但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的**，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。  

select():

select系统调用时用来让我们的程序监视多个文件句柄的状态变化的。程序会停在select这里等待，直到被监视的文件句柄有一个或多个发生了状态改变。 

**select运行机制**

select()的机制中提供一种`fd_set`的数据结构，实际上是一个long类型的数组，每一个数组元素都能与一打开的文件句柄（不管是Socket句柄,还是其他文件或命名管道或设备句柄）建立联系，建立联系的工作由程序员完成，当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读。

从流程上来看，使用select函数进行IO请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。但是，使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。

**select机制的问题**

1. 每次调用select，都需要把`fd_set`集合从用户态拷贝到内核态，如果`fd_set`集合很大时，那这个开销也很大
2. 同时每次调用select都需要在内核遍历传递进来的所有`fd_set`，如果`fd_set`集合很大时，那这个开销也很大
3. 为了减少数据拷贝带来的性能损坏，内核对被监控的`fd_set`集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)

select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：

1、 单个进程可监视的fd数量被限制，即能监听端口的大小有限。

   一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.

2、 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低：

​    当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。。

3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。

**poll：**

poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。

**它没有最大连接数的限制**，原因是它是基于链表来存储的，但是同样有一个缺点：

1、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。          

2、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

**epoll:**

 相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 

```c
struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};
```

epoll通过在Linux内核中申请一个简易的文件系统(文件系统一般用什么数据结构实现？B+树)。把原先select/poll调用分成了3个部分：

- `epoll_create`: 创建一个epoll实例，文件描述符
- `epoll_ctl`: 将监听的文件描述符添加到epoll实例中，实例代码为将标准输入文件描述符添加到epoll中
- `epoll_wait`: 等待epoll事件从epoll实例中发生， 并返回事件以及对应文件描述符

​           epoll_create创建一个epoll文件描述符，底层同时创建一个**红黑树**，和一个**就绪链表**；红黑树存储所监控的文件描述符的节点数据，就绪链表存储就绪的文件描述符的节点数据；epoll_ctl将会添加新的描述符，首先判断是红黑树上是否有此文件描述符节点，如果有，则立即返回。如果没有， 则在树干上插入新的节点，并且告知**内核注册回调函数**。当接收到某个文件描述符过来数据时，那么内核将该节点插入到就绪链表里面。epoll_wait将会接收到消息，并且将数据拷贝到用户空间，清空链表。

具体： 

epoll_create：创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

 ![这里写图片描述](https://img-blog.csdn.net/20170729203023873?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYmFpeWVfeGluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

epoll_create()函数没有什么实际作用，它仅有的一个size参数没有，它的主要功能是通过epoll_create1()函数来完成的

epoll_create1()函数 首先调用ep_alloc函数申请一个eventpoll结构，并且初始化该结构的成员

```c
// epoll的核心实现对应于一个epoll描述符  
struct eventpoll {  
    spinlock_t lock;  
    struct mutex mtx;  
    wait_queue_head_t wq; // sys_epoll_wait() 等待在这里  
    // f_op->poll()  使用的, 被其他事件通知机制利用的wait_address  
    wait_queue_head_t poll_wait;  
    //已就绪的需要检查的epitem 列表 
    struct list_head rdllist;  
    //保存所有加入到当前epoll的文件对应的epitem  
    struct rb_root rbr;  
    // 当正在向用户空间复制数据时, 产生的可用文件  
    struct epitem *ovflist;  
    /* The user that created the eventpoll descriptor */  
    struct user_struct *user;  
    struct file *file;  
    //优化循环检查，避免循环检查中重复的遍历
    int visited;  
    struct list_head visited_list_link;  
}

```

 接下来调用get_unused_fd_flags函数，在本进程中申请一个未使用的fd文件描述符。 

之后通过调用anon_inode_getfd函数创建一个名字为“[eventpoll]”的eventpollfs文件描述符号并将

file->private_data指定为指向前面生成的eventpoll。

anon_inode_getfile函数中首先会alloc一个file结构和一个dentry结构，然后将该file结构与一个匿名inode节点anon_inode_inode挂钩在一起，而在调用anon_inode_getfile函数申请file结构时，传入了前面申请的eventpoll结构的ep变量，申请的file->private_data会指向这个ep变量，同时，在anon_inode_getfile函数返回来后，ep->file会指向该函数申请的file结构变量。

 最后，epoll_create1调用fd_install函数，将fd与file交给关联在一起   

 epoll_ctl函数：epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。第一个参数是epoll_create()的返回值，第二个参数表示动作，用三个宏来表示：
 EPOLL_CTL_ADD：注册新的fd到epfd中；
 EPOLL_CTL_MOD：修改已经注册的fd的监听事件；
 EPOLL_CTL_DEL：从epfd中删除一个fd；

通过epoll_create生成一个eventpoll后，可以通过epoll_ctl提供的相关操作对eventpoll进行ADD，MOD，DEL操作。有一个op参数，是对epoll操作的动作（添加/修改/删除事件），ep_op_has_event(op)判断是否不是删除操作，如果op != EPOLL_CTL_DEL为true，则需要调用copy_from_user函数将用户空间传过来的event事件拷贝到内核的epds变量中。因为，只有删除操作，内核不需要使用进程传入的event事件。

epoll_ctl会通过epfd的private_data域获取需要操作的eventpoll，然后通过ep_find确认需要操作的fd是否已经在被监视的红黑树中（eventpoll->rbr）。然后使用switch根据op的类型分别作ADD（ep_insert），MOD（ep_modify），DEL（ep_remove）操作。

```c
// 对应于一个加入到epoll的文件  
struct epitem {  
    // 挂载到eventpoll 的红黑树节点  
    struct rb_node rbn;  
    // 挂载到eventpoll.rdllist 的节点  
    struct list_head rdllink;  
    // 连接到ovflist 的指针  
    struct epitem *next;  
    /* 文件描述符信息fd + file, 红黑树的key */  
    struct epoll_filefd ffd;  
    /* Number of active wait queue attached to poll operations */  
    int nwait;  
    // 当前文件的等待队列(eppoll_entry)列表  
    // 同一个文件上可能会监视多种事件,  
    // 这些事件可能属于不同的wait_queue中  
    // (取决于对应文件类型的实现),  
    // 所以需要使用链表  
    struct list_head pwqlist;  
    // 当前epitem 的所有者  
    struct eventpoll *ep;  
    /* List header used to link this item to the &quot;struct file&quot; items list */  
    struct list_head fllink;  
    /* epoll_ctl 传入的用户数据 */  
    struct epoll_event event;  
};  
```

ep_insert() 首先调用kmem_cache_alloc函数，从slab分配器里面分配一个epitem结构监听项，然后对该结构进行初始化 . 接下来，调用list_add_tail_rcu将当前监听项添加到目标文件的f_ep_links链表里面，该链表是目标文件的epoll钩子链表，所有对该目标文件进行监听的监听项都会加入到该链表里面。  然后调用ep_rbtree_insert，将epi监听项添加到ep维护的红黑树里面。
        通过传入参数event对ep内部变量event赋值。然后通过ep_set_ffd将目标文件和epitem关联。这样epitem本身就完成了和eventpoll以及被监视文件的关联。下面还需要做两个动作：将epitem插入目标文件的polllist并注册回调函数；将epitem插入eventpoll的rbtree。

 

epoll_wait()函数：等待事件的产生，类似于select()调用。参数events用来从内核得到事件的集合，**maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size**(备注：在4.1.2内核里面，epoll_create的size没有什么用），参数timeout是超时时间（毫秒，0会立即返回，小于0时将是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时

![这里写图片描述](https://img-blog.csdn.net/20170729212247508?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYmFpeWVfeGluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​       epoll_wait() 首先会检测传入参数的合法性；events指向的空间是否可写；epfd是否合法等。参数合法性检测都通过后，将通过epfd获取锁依赖的struct file，
​        然后通过file->private_data获取eventpoll。获取epoll后调用ep_poll函数完成真正的epoll_wait工作。
ep_poll()函数

​        首先是对等待时间的处理，timeout超时时间以ms为单位，timeout大于0，说明等待timeout时间后超时，如果timeout等于0，函数不阻塞，直接返回，小于0的情况，是永久阻塞，直到有事件产生才返回。

​        当没有事件产生时（(!ep_events_available(ep))为true）,调用__add_wait_queue_exclusive函数将当前进程加入到ep->wq等待队列里面，然后在一个无限for循环里面，首先调用set_current_state(TASK_INTERRUPTIBLE)，将当前进程设置为可中断的睡眠状态，然后当前进程就让出cpu，进入睡眠，直到有其他进程调用wake_up或者有中断信号进来唤醒本进程，它才会去执行接下来的代码。

​        如果进程被唤醒后，首先检查是否有事件产生，或者是否出现超时还是被其他信号唤醒的。如果出现这些情况，就跳出循环，将当前进程从ep->wp的等待队列里面移除，并且将当前进程设置为TASK_RUNNING就绪状态。

​        如果真的有事件产生，就调用ep_send_events函数，将events事件转移到用户空间里面。ep_send_events函数调用ep_scan_ready_list对ready_list进行扫描。ep_scan_ready_list首先将ep就绪链表里面的数据链接到一个全局的txlist里面，然后清空ep的就绪链表，同时还将ep的ovflist链表设置为NULL，ovflist是用单链表，是一个接受就绪事件的备份链表，当内核进程将事件从内核拷贝到用户空间时，这段时间目标文件可能会产生新的事件，这个时候，就需要将新的时间链入到ovlist里面。 仅接着，调用sproc回调函数(这里将调用ep_send_events_proc函数)将事件数据从内核拷贝到用户空间。 

在回调结束后，需要重新将ovlist链表里面的事件添加到rdllist就绪事件链表里面。

同时在最后，如果rdlist不为空（表示是否有就绪事件），并且由进程等待该事件，就调用wake_up_locked再一次唤醒内核进程处理事件的到达（流程跟前面一样，也就是将事件拷贝到用户空间）。





epoll对文件描述符的操作有两种模式：LT（level trigger）水平触发和ET（edge trigger）边缘触发。LT模式是默认模式，LT模式与ET模式的区别如下：

　　LT模式：**当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。**

　　ET模式：**当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。**

　　**ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。**



**epoll的优点：**

- select/poll每次调用都要传递所要监控的所有fd给select/poll系统调用（这意味着每次调用都要将fd列表从用户态拷贝到内核态，当fd数目很多时，这会造成低效）。而epoll每次调用epoll_wait时（作用相当于调用select/poll），不需要再传递fd列表给内核，因为已经在epoll_ctl中将需要监控的fd告诉了内核（epoll_ctl不需要每次都拷贝所有的fd，只需要进行增量式操作）。所以，在调用epoll_create之后，内核已经在内核态开始准备数据结构存放要监控的fd了。每次epoll_ctl只是对这个数据结构进行简单的维护。
- epoll向内核注册了一个文件系统，用于存储上述的被监控的fd。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的fd，这些fd会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。

- select/poll一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket是"活跃"的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对"活跃"的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。

- 当我们调用epoll_ctl往里塞入百万个fd时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的fd给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的fd外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。



**JAVA NIO**

java nio和传统I/O区别：

1. 传统I/O是面向流的，NIO是面向缓冲区的，在面向流的操作中，数据只能在一个流中连续读写，数据没有缓冲，因此字节流无法前后移动。而NIO每次都是从一个Buffer读入到一个Channel中，再从Buffer写入Channel中，因此可以方便地在缓冲区进行数据地前后移动等操作。
2. 传统I/O流操作时阻塞模式的，NIO的流操作是非阻塞模式的。NIO通过Selector监听Channel上时间的变化，在Channel上事件的变化，在Channel上有数据发生变化时通知该线程进行读写操作。

核心组件：

1. Channel

Channel和I/O中的流类似，只不过流是单向的，而Channel是双向的，既可以用来进行读操作，又可以用来写操作。NIO中Channel的主要实现由FileChannel,DatagramChannel,SocketChannel,ServerSocketChannel，分别对应文件I/O，UDP、TCP I/O，Socket Client 和 SocketServer。

2. Buffer

Buffer实际上是一个容器，其内部通过一个连续的字节数组存储在I/O上的数据，在NIO中，Channel在文件、网络上堆数据的读取或写入都必须经过Buffer。在NIO中，Buffer是一个抽象类，对不同的数据类型实现不同的Buffer操作。

3. Selector

Selector用于检测在多个Channel上是否有I/O事件发生，并对检测到的I/O事件进行响应的响应和处理。因此通过一个Selector线程就可以实现对多个Channel的管理，同时，Selector只有在Channel上有读写事件发生时，才会调用I/O函数进行读写操作，可以极大减少系统开销，提高系统的并发量。

AIO:

异步I/O，在异步I/O模型中，用户会发起一个asychronous read操作到内核，内额在接收到请求后会立刻返回一个状态，来说明请求是否发起成功，再次过程中用户线程不会发生阻塞，接着内核会等待数据准备完成并将数据复制到用户线程中，在数据复制完成后内核会返回一个信号到用户线程，通知用户线程已经完成。在异步I/O中，用户线程不需要关心I/O操作是如何进行的，只要发起一个请求，在等到通知后就说明I/O操作已经完成，直接使用数据即可。

**reactor模式**：

 Reactor模式(反应器模式)是一种处理一个或多个客户端并发交付服务请求的事件设计模式。当请求抵达后，服务处理程序使用I/O多路复用策略，然后同步地派发这些请求至相关的请求处理程序。 

**Reactor模式的角色构成(Reactor模式一共有5中角色构成)：**

- Handle(句柄或描述符，在Windows下称为句柄，在Linux下称为描述符)：本质上表示一种资源(比如说文件描述符，或是针对网络编程中的socket描述符)，是由操作系统提供的；该资源用于表示一个个的事件，事件既可以来自于外部，也可以来自于内部；外部事件比如说客户端的连接请求，客户端发送过来的数据等；内部事件比如说操作系统产生的定时事件等。它本质上就是一个文件描述符，Handle是事件产生的发源地。
- Synchronous Event Demultiplexer(同步事件分离器)：它本身是一个系统调用，用于等待事件的发生(事件可能是一个，也可能是多个)。调用方在调用它的时候会被阻塞，一直阻塞到同步事件分离器上有事件产生为止。对于Linux来说，同步事件分离器指的就是常用的I/O多路复用机制，比如说select、poll、epoll等。在Java NIO领域中，同步事件分离器对应的组件就是Selector；对应的阻塞方法就是select方法。
- Event Handler(事件处理器)：本身由多个回调方法构成，这些回调方法构成了与应用相关的对于某个事件的反馈机制。在Java NIO领域中并没有提供事件处理器机制让我们调用或去进行回调，是由我们自己编写代码完成的。Netty相比于Java NIO来说，在事件处理器这个角色上进行了一个升级，它为我们开发者提供了大量的回调方法，供我们在特定事件产生时实现相应的回调方法进行业务逻辑的处理，即，ChannelHandler。ChannelHandler中的方法对应的都是一个个事件的回调。
- Concrete Event Handler(具体事件处理器)：是事件处理器的实现。它本身实现了事件处理器所提供的各种回调方法，从而实现了特定于业务的逻辑。它本质上就是我们所编写的一个个的处理器实现。
- Initiation Dispatcher(初始分发器)：实际上就是Reactor角色。它本身定义了一些规范，这些规范用于控制事件的调度方式，同时又提供了应用进行事件处理器的注册、删除等设施。它本身是整个事件处理器的核心所在，Initiation Dispatcher会通过Synchronous Event Demultiplexer来等待事件的发生。一旦事件发生，Initiation Dispatcher首先会分离出每一个事件，然后调用事件处理器，最后调用相关的回调方法来处理这些事件。Netty中ChannelHandler里的一个个回调方法都是由bossGroup或workGroup中的某个EventLoop来调用的。

# TCP/UDP

## TCP头部

 ![img](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/55dc4e84-573d-4c13-a765-52ed1dd251f9.png)





- **序号** ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。
- **确认号** ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。
- **数据偏移** ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。
- **确认 ACK** ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。
- **同步 SYN** ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。
- **终止 FIN** ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。
- **窗口** ： 是TCP流量控制的一个手段，这里说的窗口是指接收通告窗口，它告诉对方本端的tcp接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。 

总共21字节， 可以使用tcpdump来观察tcp头部信息 

 

## TCP三次握手

 ![img](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/e92d0ebc-7d46-413b-aec1-34a39602f787.png)

假设 A 为客户端，B 为服务器端。

- 首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。

- A 向 B 发送连接请求报文，SYN=1，ACK=0，选择一个初始的序号 x。
- B 收到连接请求报文，如果同意建立连接，则向 A 发送连接确认报文，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。
- A 收到 B 的连接确认报文后，还要向 B 发出确认，确认号为 y+1，序号为 x+1。
- B 收到 A 的确认后，连接建立。

socket流程

 ![img](https://images0.cnblogs.com/blog/349217/201312/05232335-fb19fc7527e944d4845ef40831da4ec2.png) 



与socket对应关系：

 ![这里写图片描述](https://img-blog.csdn.net/20170804111956459?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZnV0dXJld3U=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

connect时，触发了连接请求，向服务器发送了SYN J包，这时connect进入阻塞状态；服务器监听到连接请求，即收到SYN J包，调用accept函数接收请求向客户端发送SYN K ，ACK J+1，这时accept进入阻塞状态；客户端收到服务器的SYN K ，ACK J+1之后，这时connect返回，并对SYN K进行确认；服务器收到ACK K+1时，accept返回，至此三次握手完毕，连接建立。



**三次握手的原因**

两个角度：

1. 第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。

客户端发送的连接请求如果在网络中滞留，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。



2. TCP是可靠传输， 为了实现**可靠数据传输**， TCP 协议的通信双方， 都必须维护一个序列号， 以标识发送出去的数据包中， 哪些是已经被对方收到的。 **三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值的必经步骤。**
   如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， **另一方选择的序列号则得不到确认** 

## TCP四次挥手

 ![img](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/f87afe72-c2df-4c12-ac03-9b8d581a8af8.jpg) 



1）客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。
2）服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。
3）客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。
4）服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。
5）客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。
6）服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。
TIME-WAIT   CLOSE-WAIT是重点

**四次挥手的原因**

由于TCP是全双工协议，TCP要保证传输两端数据发送完毕。客户端和服务端分别的两次挥手，也就是客户端和服务端**分别释放**连接的过程。客户端在发送完最后一次确认之后，还要**等待2MSL**的时间。主要有两个原因，**一个**是为了让B能够按照**正常**步骤**进入CLOSED**状态，**二是**为了防止**已经失效**的请求连接报文出现在下次连接中。

1）、由于**客户端最后一个ACK可能会丢失**，这样B就无法正常进入CLOSED状态。于是B会重传请求释放的报文，而此时A如果已经关闭了，那就收不到B的重传请求，就会**导致B不能正常释放**。而如果A还在等待时间内，就会收到B的重传，然后进行应答，这样B就可以进入CLOSED状态了。

2）、在这2MSL等待时间里面，本次连接的所有的报文都已经**从网络中消失**，从而不会出现在下次连接中。

**TIME_WAIT**

客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由：

- 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。
- 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。

**为什么是2MSL？**

MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。从客户端发送确认包开始计时，ACK从A到B最多经过1MSL，超过这个时间B会重发FIN，B重发的FIN最多经过1MSL到达A。

结论：如果B重发了FIN，且网络没有故障(重发的FIN被丢弃或错误转发)，那么A一定能在2MSL之内收到该FIN，因此A只需要等待2MSL。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 



## TCP拥塞控制

拥塞控制是防止过多的数据注入网络中，使得网络中路由器或链路不致过载，有一个前提是，网络能够承受现有的网络负荷，是一个**全局性**过程；流量控制是指点对点通信的控制，做的是抑制发送端发送数据的速率，便于接收端来得及接收。

主要有四种算法：**慢开始、拥塞避免、快重传、快恢复**。

1、慢开始和拥塞避免

基于窗口的拥塞控制，在发送方维护一个**拥塞窗口**（cwnd），大小等于发送窗口，通过出现了**超时**来判断网络出现拥塞。慢开始的思路是一开始发送方发送一个字节，在收到接收方的确认，然后发送的字节数量增大一倍（也就是按照**指数**增长的速率），从小到大逐步增大cwnd，直到cwnd 达到**慢开始门限**（ssthresh），停止慢开始算法，使用拥塞避免算法，拥塞避免算法思路是增长速率变为**线性**增长，也就是每经过一个往返时间RTT就把发送方的cwnd加1，所以综上：

当cwnd < ssthresh ，使用慢开始算法；
 当cwnd = ssthresh，可以使用慢开始算法，也可以使用拥塞算法；
 当cwnd > ssthresh，使用拥塞算法；

2、快重传和快恢复

通过上面两个算法可以使得网络传输速率一直增大，直到出现超时，这时候需要将cwnd重新调整到1个字节开始，使用慢开始算法，同时需要将慢开始门限ssthresh调整为cwnd（超时点）的一半，继续执行慢开始、拥塞避免算法。如果收到**3-ACK**（发送方一连接收到3个对同一个报文段的重复确认），这种可能的情况是，并不是发生了拥塞，可能是报文丢失，所以发送方不执行慢开始算法，直接使用快重传算法，立即发送缺失的报文段。同时执行快恢复算法，将门限值（ssthresh）调整为此时cwnd的一半，并执行拥塞避免算法。

## TCP可靠传输

TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 

TCP 滑动窗口
滑动窗口协议是**传输层进行流控**的一种措施。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。

发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。

接收方窗口的大小通过ACK来发送：ACK有两个非常重要的信息：
一是期望接收到的下一字节的序号n，该n代表接收方已经接收到了前n-1字节数据，此时如果接收方收到第n+1字节数据而不是第n字节数据，接收方是不会发送序号为n+2的ACK的。

二是当前的窗口大小m，如此发送方在接收到ACK包含的这两个数据后就可以计算出还可以发送多少字节的数据给对方。发送方根据收到ACK当中的期望收到的下一个字节的序号n以及窗口m，还有当前已经发送的字节序号x，算出还可以发送的字节数。



**滑动窗口实现面向流的可靠性**

1）最基本的传输可靠性来源于“确认重传”机制。

2）TCP的滑动窗口的可靠性也是建立在“确认重传”基础上的。

3）发送窗口只有收到对端对于本段发送窗口内字节的ACK确认，才会移动发送窗口的左边界。

4）接收窗口只有在前面所有的段都确认的情况下才会移动左边界。当在前面还有字节未接收但收到后面字节的情况下，窗口不会移动，并不对后续字节确认。以此确保对端会对这些数据重传。

## TCP粘包拆包心跳包

 由于`TCP`传输协议是面向字节流的传输协议，没有消息保护边界，所以发送方发送的多个数据包，接收方应用层不知如何区分，可能会被当成一个包来处理，这就是粘包；或者，发送方将一个打包分成多个小包发送，而接收方将它们当成多个包进行处理，这就是拆包。UDP是面向报文的，在头部有报文长度，不会用粘包问题。

解决方法：

**（1）定长协议**

  定长协议，顾名思义，就是应用层需要发送的每份数据，长度都是固定的。比如说，将数据长度定义为`1024`字节，所有不满足`1024`字节的数据，可以通过补`0`进行填充。而接收方每次读取`1024`字节，就可以正确区分每一份数据。

- 发送方：每次发送固定长度的数据，若数据长度不够，就使用其他字符填充；
- 接收方：每次读取固定字节的数据；

  这种方式并不好，对数据进行填充，完全就是一种浪费带宽的行为，而且处理起来也麻烦。

 **（2）特殊字符分隔**

  我们可以为每一份数据，添加起始字符和结束字符，这样就可以区分了。

- 发送方：对数据的开始和结束分别加上相应的标记字符；
- 接收方：根据标记字符，逐个读取每一份数据；

  当然，有时候我们并不确定应该选择哪个字符作为标记字符，因为不确定这个字符是否原本就在数据中包含。此时我们可以对数据进行转码，比如说将数据转成`Base64`编码，而`Base64`只有`64`种字符，然后我们就可以使用这`64`种之外的字符作为标记。

 **（3）变长协议**

  这种实现也是比较简单的，对于应用层的报文，可以将它分为报文头部以及报文体，而我们可以在报文头中指定当前报文中数据的长度，这样，接收方就能根据长度，正确地拆分多个粘在一起的数据了。

- 发送方：将发送的报文分为头部和实体，在头部中指明实体中数据的长度；
- 接收方：根据报文头部中的信息，正确地区分多个数据；



心跳包：

心跳包：它像心跳一样每隔固定时间发一次，以此来告诉服务器，这个客户端还活着。事实上这是为了保持长连接，至于这个包的内容，是没有什么特别规定的，不过一般都是很小的包，或者只包含包头的一个空包。 一般是客户端。服务器也可以定时轮询发心跳下去。  

## UDP头部

 ![img](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/d4c3a4a1-0846-46ec-9cc3-eaddfca71254.jpg)‘首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 

## UDPTCP区别

- 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。
- 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。

## 设计一个可靠的UDP

帧同步中的UDP

帧同步方案中逻辑帧一般都在8~16帧左右，一般都在12帧以上，这要的网络传输频率决定了不可能采用TCP协议，那么如果采用UDP会出现什么问题呢？

1. 丢包，帧同步中逻辑帧在每个Client上一定是一致的，也就是说决定不能出现丢帧的情况，

2. 数据完整性，UDP协议头部虽然有16位的校验和，但是IPv4并不强制执行，也就是说UDP无法抱枕数据的完整性

3. 乱序。 UDP并不保证数据的顺序，故可能出现数据包乱序问题

以上问题任何一项都会导致Client在逻辑帧上出现不同步问题，为了解决这个问题就必须引入RUDP概念，这里的RUDP主要是解决上述的问题，并不某个RFC定义的规范。

RUDP初步

首先思考RUDP需要解决哪些问题，然后根据问题加上必要的包头字段。

1. 数据完整性 –> 加上一个16或者32位的CRC验证字段

2. 乱序 –> 加上一个数据包序列号SEQ

3. 丢包 –> 需要确认和重传机制，就是和Tcp类似的Ack机制

在思考一下既然是自定义协议是不是需要一个协议号字段来过滤一些非法包，那么又可以加入一个新字段：

4. 协议字段 –> protol 字段，标识当前使用协议

 ![5@66$I~G(YOAI8@~VIGVJA2](https://images2015.cnblogs.com/blog/727571/201707/727571-20170711220859025-658050635.png) 

## tcp和udp的优点

* TCP的优点： 可靠，稳定 TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 TCP的缺点： 慢，效率低，占用系统资源高，易被攻击 TCP在传递数据之前，要先建连接，这会消耗时间，而且在数据传递时，确认机制、重传机制、拥塞控制机制等都会消耗大量的时间，而且要在每台设备上维护所有的传输连接，事实上，每个连接都会占用系统的CPU、内存等硬件资源。 而且，因为TCP有确认机制、三次握手机制，这些也导致TCP容易被人利用，实现DOS、DDOS、CC等攻击。
* UDP的优点： 快，比TCP稍安全 UDP没有TCP的握手、确认、窗口、重传、拥塞控制等机制，UDP是一个无状态的传输协议，所以它在传递数据时非常快。没有TCP的这些机制，UDP较TCP被攻击者利用的漏洞就要少一些。但UDP也是无法避免攻击的，比如：UDP Flood攻击…… UDP的缺点： 不可靠，不稳定 因为UDP没有TCP那些可靠的机制，在数据传递时，如果网络质量不好，就会很容易丢包。 基于上面的优缺点，那么： 什么时候应该使用TCP： 当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。 在日常生活中，常见使用TCP协议的应用如下： 浏览器，用的HTTP FlashFXP，用的FTP Outlook，用的POP、SMTP Putty，用的Telnet、SSH QQ文件传输。什么时候应该使用UDP： 当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 比如，日常生活中，常见使用UDP协议的应用如下： QQ语音 QQ视频 TFTP。

# HTTP

## 输入一个url之后的流程

1. DNS解析

2.  解析过程 1. 浏览器搜索自己的 DNS 缓存（维护一张域名与 IP 地址的对应表）；

   2. 搜索操作系统中的 DNS 缓存（维护一张域名与 IP 地址的对应表）；

   3. 搜索操作系统的 hosts 文件（ Windows 环境下，维护一张域名与 IP 地址的对应表）；

   4. 操作系统将域名发送至 LDNS（本地区域名服务器，如果你在学校接入互联网，则 LDNS 服务器就在学校，如果通过电信接入互联网，则 LDNS 服务器就在你当地的电信那里。）LDNS 查询自己的 DNS 缓存（一般查找成功率在 80% 左右），查找成功则返回结果，失败则发起一个迭代 DNS 解析请求；

   - LDNS 向 Root Name Server （根域名服务器，其虽然没有每个域名的的具体信息，但存储了负责每个域，如 com、net、org等的解析的顶级域名服务器的地址）发起请求，此处，Root Name Server 返回 com 域的顶级域名服务器的地址；
   - LDNS 向 com 域的顶级域名服务器发起请求，返回 baidu.com 域名服务器地址；
   - LDNS 向 baidu.com 域名服务器发起请求，得到 [www.baidu.com](http://www.baidu.com/) 的 IP 地址；

   5. LDNS 将得到的 IP 地址返回给操作系统，同时自己也将 IP 地址缓存起来；

   6. 操作系统将 IP 地址返回给浏览器，同时自己也将 IP 地址缓存起来；

   7. 至此，浏览器已经得到了域名对应的 IP 地址。

2. TCP连接：三次握手

3. 发送http请求

​       建立了TCP连接之后，发起一个http请求。一个典型的 http request header 一般需要包括请求的`Content-Type`，`Accept-Language`，甚至请求的`Cookie`信息也是在`Header`中传输。 

4. 返回http响应

​       服务器接受并处理完请求，返回 HTTP 响应，一个响应报文格式基本等同于请求报文，由响应行、响应头、空行、实体组成。 

5. 浏览器解析渲染页面

6. 断开连接：四次挥手



## cookie session

**cookie **

一个用户的所有请求操作都应该属于同一个会话。而Web应用程序是使用HTTP协议传输数据的，由于HTTP协议是无状态的协议。一旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建立新的连接。这就意味着服务器无法从连接上跟踪会话。

Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。**客户端浏览器会把Cookie保存起来**。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。

Cookie在客户端是由浏览器来管理的，具有不可跨域名性(浏览器访问Google只会携带Google的Cookie，而不会携带Baidu的Cookie。Google也只能操作Google的Cookie，而不能操作Baidu的Cookie。）。

**session**

**Session是服务器端使用的一种记录客户端状态的机制**。Session是记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session保存在服务器上。客户端浏览器访问服务器的时候，**服务器把客户端信息以某种形式记录在服务器上，这就是Session**。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。

**cookie和session区别**

最明显的不同是一个在客户端一个在服务端。因为Cookie存在客户端所以用户可以看见，所以也可以编辑伪造，不是十分安全。

Session过多的时候会消耗服务器资源，所以大型网站会有专门的Session服务器，而Cookie存在客户端所以没什么问题。另外，Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式（session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie ，同时 session 也会失效；session也可以通过URL中传递session_id实现）。

## 请求头 响应头

 1）请求(客户端->服务端[request]) 

一个HTTP请求报文由请求行（request line）、请求头部（headers）、空行（blank line）和请求数据（request body）4个部分组成。

  GET(请求的方式) /newcoder/hello.html(请求的目标资源) HTTP/1.1(请求采用的协议和版本号) 
  Accept: */*(客户端能接收的资源类型) 
  Accept-Language: en-us(客户端接收的语言类型) 
  Connection: Keep-Alive(维护客户端和服务端的连接关系) 
  Host: localhost:8080(连接的目标主机和端口号) 
  Referer: http://localhost/links.asp(告诉服务器我来自于哪里) 
  User-Agent: Mozilla/4.0(客户端版本号的名字) 
  Accept-Encoding: gzip, deflate(客户端能接收的压缩数据的类型) 
  If-Modified-Since: Tue, 11 Jul 2000 18:23:51 GMT(缓存时间)  
  Cookie(客户端暂存服务端的信息) 
  Date: Tue, 11 Jul 2000 18:23:51 GMT(客户端请求服务端的时间) 



 2）响应(服务端->客户端[response])

报文由状态行（status line）、相应头部（headers）、空行（blank line）和响应数据（response body）4个部分组成。

  HTTP/1.1(响应采用的协议和版本号) 200(状态码) OK(描述信息)
  Location: http://www.baidu.com(服务端需要客户端访问的页面路径) 
  Server:apache tomcat(服务端的Web服务端名)
  Content-Encoding: gzip(服务端能够发送压缩编码类型) 
  Content-Length: 80(服务端发送的压缩数据的长度) 
  Content-Language: zh-cn(服务端发送的语言类型) 
  Content-Type: text/html; charset=GB2312(服务端发送的类型及采用的编码方式)
  Last-Modified: Tue, 11 Jul 2000 18:23:51 GMT(服务端对该资源最后修改的时间)
  Refresh: 1;url=http://www.it315.org(服务端要求客户端1秒钟后，刷新，然后访问指定的页面路径)
  Content-Disposition: attachment; filename=aaa.zip(服务端要求客户端以下载文件的方式打开该文件)
  Transfer-Encoding: chunked(分块传递数据到客户端）  
  Set-Cookie:SS=Q0=5Lb_nQ; path=/search(服务端发送到客户端的暂存数据)
  Expires: -1//3种(服务端禁止客户端缓存页面数据)
  Cache-Control: no-cache(服务端禁止客户端缓存页面数据)  
  Pragma: no-cache(服务端禁止客户端缓存页面数据)  
  Connection: close(1.0)/(1.1)Keep-Alive(维护客户端和服务端的连接关系)  
  Date: Tue, 11 Jul 2000 18:23:51 GMT(服务端响应客户端的时间) 

## HTTP1.0/1.1/2.0

**HTTP1.0 HTTP 1.1主要区别**

1. **缓存处理**，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
2. **带宽优化**，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
3. **Host头处理**，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
4. **长连接**，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。

**HTTP1.1 HTTP 2.0主要区别**

1. **二进制分帧**

`HTTP2.0`通过在应用层和传输层之间增加一个二进制分帧层, 把原来`HTTP1.x`的`header`和`body`部分用`frame`重新封装了一层而已。并通过二进制分帧实现了多路复用。 

2. **多路复用**

​       HTTP2.0使用了多路复用的技术，所有的`HTTP2.0`通信都在一个`TCP`连接上完成，这个连接可以承载任意数量的双向数据流。 每个数据流以消息的形式发送，而消息由一或多个帧组成。这些帧可以乱序发送，然后再根据每个帧头部的流标识符（`stream id`）重新组装。  多路复用（连接共享）可能会导致关键请求被阻塞。`HTTP2.0`里每个数据流都可以设置优先级和依赖，优先级高的数据流会被服务器优先处理和返回给客户端。

​        当然HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。TCP连接有一个预热和保护的过程，先检查数据是否传送成功，一旦成功过，则慢慢加大传输速度。因此对应瞬时并发的连接，服务器的响应就会变慢。所以最好能使用一个建立好的连接，并且这个连接可以支持瞬时并发的请求。

3. **数据压缩**
   HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。

4. **服务器推送**
   当我们对支持HTTP2.0的web server请求数据的时候，服务器会顺便把一些客户端需要的资源一起推送到客户端，免得客户端再次创建连接发送请求到服务器端获取。这种方式非常合适加载静态资源。

​       服务器端推送的这些资源其实存在客户端的某处地方，客户端直接从本地加载这些资源就可以了，不用走网络，速度就会快很多。


## HTTPS

 超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。

​    为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS，为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL/TLS协议，SSL/TLS依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。

​    HTTPS协议是由SSL/TLS+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全

​    HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。

HTTPS和HTTP的主要区别

        1、https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。
        2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl/tls加密传输协议
        3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
        4、http的连接很简单，是无状态的；HTTPS协议是由SSL/TLS+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。但是成本和连接时间会增加。
客户端在使用HTTPS方式与Web服务器通信时的步骤
　（1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。

　（2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。

　（3）客户端的浏览器与Web服务器开始协商SSL/TLS连接的安全等级，也就是信息加密的等级。

　（4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。

　（5）Web服务器利用自己的私钥解密出会话密钥。

　（6）Web服务器利用会话密钥加密与客户端之间的通信。


## 状态码

 状态码的职责是当客户端向服务器发送请求时，描述返回的请求结果。借助状态码，用户可以知道服务器端是正常处理了请求还是出现了错误。

状态码的类别：

 	       类别	                                               原因
1XX	Informational（信息性状态码）	接受的请求正在处理
2XX	Success（成功状态码）	               请求正常处理完毕
3XX	Redirection（重定向状态码）	     需要进行附加操作以完成请求
4XX	Client Error（客户端错误状态码     服务器无法处理请求
5XX	Server Error（服务器错误状态码）服务器处理请求出错


 **2XX——表明请求被正常处理了**

1、200 OK：请求已正常处理。

2、204 No Content：请求处理成功，但没有任何资源可以返回给客户端，一般在只需要从客户端往服务器发送信息，而对客户端不需要发送新信息内容的情况下使用。

3、206 Partial Content：是对资源某一部分的请求，该状态码表示客户端进行了范围请求，而服务器成功执行了这部分的GET请求。响应报文中包含由Content-Range指定范围的实体内容。3XX——表明浏览器需要执行某些特殊的处理以正确处理请求

 **3XX——表明浏览器需要执行某些特殊的处理以正确处理请求**

4、301 Moved Permanently：资源的uri已更新，你也更新下你的书签引用吧。永久性重定向，请求的资源已经被分配了新的URI，以后应使用资源现在所指的URI。

5、302 Found：资源的URI已临时定位到其他位置了，姑且算你已经知道了这个情况了。临时性重定向。和301相似，但302代表的资源不是永久性移动，只是临时性性质的。换句话说，已移动的资源对应的URI将来还有可能发生改变。

6、303 See Other：资源的URI已更新，你是否能临时按新的URI访问。该状态码表示由于请求对应的资源存在着另一个URL，应使用GET方法定向获取请求的资源。303状态码和302状态码有着相同的功能，但303状态码明确表示客户端应当采用GET方法获取资源，这点与302状态码有区别。

当301,302,303响应状态码返回时，几乎所有的浏览器都会把POST改成GET，并删除请求报文内的主体，之后请求会自动再次发送。

7、304 Not Modified：资源已找到，但未符合条件请求。该状态码表示客户端发送附带条件的请求时（采用GET方法的请求报文中包含If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since中任一首部）服务端允许请求访问资源，但因发生请求未满足条件的情况后，直接返回304.。

**4XX——表明客户端是发生错误的原因所在。**

9、400 Bad Request：服务器端无法理解客户端发送的请求，请求报文中可能存在语法错误。

10、401 Unauthorized：该状态码表示发送的请求需要有通过HTTP认证（BASIC认证，DIGEST认证）的认证信息。

11、403 Forbidden：不允许访问那个资源。该状态码表明对请求资源的访问被服务器拒绝了。（权限，未授权IP等）

12、404 Not Found：服务器上没有请求的资源。路径错误等。

**5XX——服务器本身发生错误**

13、500 Internal Server Error：该状态码表明服务器端在执行请求时发生了错误。

14、503 Service Unavailable：该状态码表明服务器暂时处于超负载或正在停机维护，现在无法处理请求。

# 算法

## 红黑树

红黑树是基于二叉搜索树扩展而来，对于TreeNode来说排序的依据是结点的hash值，若相等然后比较key值，若key不能比较或是相等则根据hash值，左儿子的hash值小于等于父亲，右儿子的hash值大于父亲。TreeNode 保有红黑树的性质：

1. 每个结点都是红色的或者是黑色的
2. 根结点是黑色的
3. 每个叶结点NIL是黑色的，但是通常我们不考虑NIL叶结点。
4. 如果一个结点是红色的，它的两个子结点都是黑色的
5. 每个结点到叶结点的简单路径上，均包含相同数目的黑色结点，这个属性被称为黑高，记作bh(x)



        TreeNode<K,V> parent;  //父亲结点
        TreeNode<K,V> left;    //左儿子
        TreeNode<K,V> right;   //右儿子
        TreeNode<K,V> prev;    //前方结点
        boolean red;//是否是红色
## TOPK

​    第一种的方法是将数据全部排序，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为O（nlogn），如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。

​    第二种方法为局部淘汰法，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。

​    第三种方法是分治法，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100*10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^6*4=4MB，一共需要101次这样的比较。

​    第四种方法是Hash法。如果这1亿个书里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。

​    第五种方法采用最小堆。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。

**（1）单机+单核+足够大内存**
        如果需要查找10亿个查询次（每个占8B）中出现频率最高的10个，考虑到每个查询词占8B，则10亿个查询次所需的内存大约是10^9 * 8B=8GB内存。如果有这么大内存，直接在内存中对查询次进行排序，顺序遍历找出10个出现频率最大的即可。这种方法简单快速，使用。然后，也可以先用HashMap求出每个词出现的频率，然后求出频率最大的10个词。

**（2）单机+多核+足够大内存**
        这时可以直接在内存总使用Hash方法将数据划分成n个partition，每个partition交给一个线程处理，线程的处理逻辑同（1）类似，最后一个线程将结果归并。

​    该方法存在一个瓶颈会明显影响效率，即数据倾斜。每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。而针对此问题，解决的方法是，将数据划分成c×n个partition（c>1），每个线程处理完当前partition后主动取下一个partition继续处理，知道所有数据处理完毕，最后由一个线程进行归并。

**（3）单机+单核+受限内存**
        这种情况下，需要将原数据文件切割成一个一个小文件，如次啊用hash(x)%M，将原文件中的数据切割成M小文件，如果小文件仍大于内存大小，继续采用Hash的方法对数据文件进行分割，知道每个小文件小于内存大小，这样每个文件可放到内存中处理。采用（1）的方法依次处理每个小文件。

**（4）多机+受限内存**
        这种情况，为了合理利用多台机器的资源，可将数据分发到多台机器上，每台机器采用（3）中的策略解决本地的数据。可采用hash+socket方法进行数据分发。

​    从实际应用的角度考虑，（1）（2）（3）（4）方案并不可行，因为在大规模数据处理环境下，作业效率并不是首要考虑的问题，算法的扩展性和容错性才是首要考虑的。算法应该具有良好的扩展性，以便数据量进一步加大（随着业务的发展，数据量加大是必然的）时，在不修改算法框架的前提下，可达到近似的线性比；算法应该具有容错性，即当前某个文件处理失败后，能自动将其交给另外一个线程继续处理，而不是从头开始处理。

​    top K问题很适合采用MapReduce框架解决，用户只需编写一个Map函数和两个Reduce 函数，然后提交到Hadoop（采用Mapchain和Reducechain）上即可解决该问题。具体而言，就是首先根据数据值或者把数据hash(MD5)后的值按照范围划分到不同的机器上，最好可以让数据划分后一次读入内存，这样不同的机器负责处理不同的数值范围，实际上就是Map。得到结果后，各个机器只需拿出各自出现次数最多的前N个数据，然后汇总，选出所有的数据中出现次数最多的前N个数据，这实际上就是Reduce过程。对于Map函数，采用Hash算法，将Hash值相同的数据交给同一个Reduce task；对于第一个Reduce函数，采用HashMap统计出每个词出现的频率，对于第二个Reduce 函数，统计所有Reduce task，输出数据中的top K即可。

​    直接将数据均分到不同的机器上进行处理是无法得到正确的结果的。因为一个数据可能被均分到不同的机器上，而另一个则可能完全聚集到一个机器上，同时还可能存在具有相同数目的数据。

 在海量数据中查找出**重复出现**的元素或者去除重复出现的元素的问题。一般可以通过**位图法**实现。例如，已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。

​    本题最好的解决方法是通过使用位图法来实现。8位整数可以表示的最大十进制数值为99999999。如果每个数字对应于位图中一个bit位，那么存储8位整数大约需要99MB。因为1B=8bit，所以99Mbit折合成内存为99/8=12.375MB的内存，即可以只用12.375MB的内存表示所有的8位数电话号码的内容。



# 设计模式

## 设计原则

**1、开闭原则（Open close principle）**

解释：对扩展开放，对修改关闭。

在程序需要进行拓展的时候，不能去修改原有的代码，目标是实现一个热插拔的效果。

简言之，是为了使程序的扩展性好，易于维护和升级。

想要达到这样的效果，我们需要使用接口和抽象类。

**2、里氏代换原则（Liskov Substitution Principle）**

里氏代换原则是面向对象设计的基本原则之一。

里氏代换原则：任何父类可以出现的地方，子类一定可以出现。

里氏替换原则是继承复用的基石，只有当子类可以替换掉父类，且软件的功能不受到影响时，父类才能真正被复用，而子类也可以再父类的基础上增加新的行为。

里氏代换原则是对开闭原则的补充。 实现开闭原则的关键步骤就是抽象化，而父类与子类的继承关系就是抽象化的具体体现，所以里氏代换原则是对实现抽象化的具体步骤的规范。

**3、依赖倒转原则（Dependence Inversion Principle）**

依赖倒转原则是开闭原则的基础。

依赖倒转的具体内容：针对接口编程，依赖于抽象而不依赖于具体。

**4、接口隔离原则（Interface Segregation Principe）**

接口隔离原则：使用多个隔离的接口，优于使用单个接口。

另一层意思：降低类之间的耦合度。

由此可见，其实设计模式就是从大型软件架构出发、便于升级和维护的软件设计思想，它强调降低依赖，降低耦合。

**5、迪米特法则 又称最少知道原则（Demeter Principle）**

迪米特原则是指一个实体应当尽量少地与其他实体之间发生相互作用，是的系统功能模块相对独立。

**6、合成复用原则（Composite Reuse Principle）**

合成复用原则是指：尽量使用合成聚合方式，而不是使用继承。

**7、单一职责原则（Single Responsibility Principe）**

单一职责原则：就一个类而言，应该仅有一个因其他变化的原因。

如果一个类承担的职责过多，就等于把这些职责耦合在一起，一个职责的变化可能会削弱或者抑制这个类完成其他职责的能力。这种耦合会导致脆弱的设计，当变化的发生时，设计会遭受到意想不到的破坏。

软件设计真正要做的许多内容，就是发现职责并把那些职责相互分离。

如果你能够想到多于一个的动机去改变一个类，那么这个类就具有多于一个的职责。